{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Issue 7/10\n",
    "1. Get more dataset of mental state and validate with 1246_gru_tested.h5\n",
    "2. Move onto real-time classification with the fully trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mne\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## Feature extraction method\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Using a person's 3 different mental states, relaxed, neutral, concetrating<br>\n",
    "- Sampling rate: 256 Hz (256 samples in 1 second period) \n",
    "- `trials = 3, channels = 4, samples = 15204 * 0.75, kernels = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo 1, using a subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_a_relaxed shape: (15204, 6)\n",
      "sub_a_neutral shape: (15204, 6)\n",
      "sub_a_concentrating shape: (15192, 6)\n"
     ]
    }
   ],
   "source": [
    "sub_a_relaxed = pd.read_csv(os.getcwd() + '/dataset/original_data/subjecta-relaxed-1.csv')\n",
    "print('sub_a_relaxed shape: {}'.format(sub_a_relaxed.shape))\n",
    "\n",
    "sub_a_neutral = pd.read_csv(os.getcwd() + '/dataset/original_data/subjecta-neutral-1.csv')\n",
    "print('sub_a_neutral shape: {}'.format(sub_a_neutral.shape))\n",
    "\n",
    "sub_a_concentrating = pd.read_csv(os.getcwd() + '/dataset/original_data/subjecta-concentrating-1.csv')\n",
    "print('sub_a_concentrating shape: {}'.format(sub_a_concentrating.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo 2, using the whole dataset\n",
    "- Matched the number of samples to 888, the df with the lowest number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjectb-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectd-neutral-1.csv, shape = ((888, 4))\n",
      "subjectd-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectc-relaxed-1.csv, shape = ((888, 4))\n",
      "subjecta-neutral-2.csv, shape = ((888, 4))\n",
      "subjectd-relaxed-2.csv, shape = ((888, 4))\n",
      "subjecta-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectb-neutral-2.csv, shape = ((888, 4))\n",
      "subjectb-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectd-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectd-neutral-2.csv, shape = ((888, 4))\n",
      "subjectc-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectb-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectc-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectb-neutral-1.csv, shape = ((888, 4))\n",
      "subjecta-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectb-relaxed-1.csv, shape = ((888, 4))\n",
      "subjecta-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectc-neutral-2.csv, shape = ((888, 4))\n",
      "subjecta-neutral-1.csv, shape = ((888, 4))\n",
      "subjectd-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectc-neutral-1.csv, shape = ((888, 4))\n",
      "subjecta-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectc-concentrating-1.csv, shape = ((888, 4))\n"
     ]
    }
   ],
   "source": [
    "directory = os.getcwd() + '/dataset/original_data/'\n",
    "files = os.listdir(directory)\n",
    "random.shuffle(files)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for filename in files:\n",
    "#     if 'subject' in filename:\n",
    "#         df = pd.read_csv(directory + filename)\n",
    "#         df = df.drop(columns=['timestamps','Right AUX'])\n",
    "#         if 'relaxed' in filename:\n",
    "#             labels.append(0)\n",
    "#         elif 'neutral' in filename:\n",
    "#             labels.append(1)\n",
    "#         elif 'concentrating' in filename:\n",
    "#             labels.append(2)\n",
    "#         data.append(df)\n",
    "        \n",
    "#         print('{}, shape = ({})'.format(filename, df.shape))\n",
    "        \n",
    "    if 'subject' in filename:\n",
    "        df = pd.read_csv(directory + filename)\n",
    "        df = df.drop(columns=['timestamps','Right AUX'])\n",
    "        df = df[:888]\n",
    "        if 'relaxed' in filename:\n",
    "            labels.append(0)\n",
    "        elif 'neutral' in filename:\n",
    "            labels.append(1)\n",
    "        elif 'concentrating' in filename:\n",
    "            labels.append(2)\n",
    "        data.append(df)\n",
    "        if len(df) < 15192:\n",
    "            print('{}, shape = ({})'.format(filename, df.shape))\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4, 888)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  -6.348,  -12.207,    1.953, ...,   -2.441,   -2.93 ,\n",
       "          -11.719],\n",
       "        [  51.758,   55.664,   49.805, ...,   25.879,   26.855,\n",
       "           26.367],\n",
       "        [ -24.414,  -65.918,   61.523, ...,   73.242, -105.957,\n",
       "         -112.305],\n",
       "        [   4.395,    3.906,    8.789, ...,   -5.859,   -5.371,\n",
       "           -5.859]],\n",
       "\n",
       "       [[  40.527,   39.062,   29.297, ...,   59.57 ,   54.688,\n",
       "           38.574],\n",
       "        [  39.062,   33.691,   35.645, ...,   41.016,   39.062,\n",
       "           36.133],\n",
       "        [  10.742,    1.465,   -2.93 , ...,   50.293,   33.203,\n",
       "           19.531],\n",
       "        [  31.738,   36.133,   31.25 , ...,   41.504,   46.387,\n",
       "           35.645]],\n",
       "\n",
       "       [[  20.508,   -1.465,   -0.977, ...,   17.09 ,   19.531,\n",
       "           -0.488],\n",
       "        [  12.207,   15.625,   15.625, ...,   16.602,   16.602,\n",
       "           11.719],\n",
       "        [  25.391,   35.156,   36.133, ...,   15.625,   17.578,\n",
       "           13.184],\n",
       "        [  11.23 ,   -0.488,    4.883, ...,    5.371,   10.742,\n",
       "            8.789]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  13.672,   21.484,   33.203, ...,   45.898,   30.273,\n",
       "           26.367],\n",
       "        [  35.645,   30.762,   29.785, ...,   29.785,   29.785,\n",
       "           30.273],\n",
       "        [  32.227,   24.902,   29.297, ...,   25.879,   25.391,\n",
       "           19.043],\n",
       "        [   5.371,   14.16 ,   19.043, ...,   31.738,   28.809,\n",
       "           23.438]],\n",
       "\n",
       "       [[ -14.648,   -9.277,  -11.719, ...,   56.641,   56.641,\n",
       "           54.199],\n",
       "        [  28.809,   27.344,   28.32 , ...,   14.648,   15.625,\n",
       "           14.16 ],\n",
       "        [  33.203,   29.297,   28.32 , ...,   21.484,   19.531,\n",
       "           15.625],\n",
       "        [   0.   ,   -0.488,    0.   , ...,   40.527,   39.062,\n",
       "           36.133]],\n",
       "\n",
       "       [[  20.996,   40.039,   48.34 , ...,   33.203,   47.852,\n",
       "           36.621],\n",
       "        [  23.926,   22.949,   24.902, ...,   37.109,   35.156,\n",
       "           35.156],\n",
       "        [  29.297,   28.809,   31.25 , ...,   24.902,   20.508,\n",
       "           18.066],\n",
       "        [  20.02 ,   21.973,   28.32 , ...,   20.996,   26.367,\n",
       "           21.973]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([np.transpose(df) for df in data])\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label\n",
    "- 0.0 = relaxed<br>\n",
    "- 1.0 = neutral<br>\n",
    "- 2.0 = concetrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0, 1,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Convert data to NHWC (trials, channels, samples, kernels) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "- Drop unnecessary columns = `['timestamps','Right AUX']`\n",
    "- Match the number of samples = `15192`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_a_relaxed = sub_a_relaxed.drop(columns=['timestamps','Right AUX'])\n",
    "sub_a_neutral = sub_a_neutral.drop(columns=['timestamps','Right AUX'])\n",
    "sub_a_concentrating = sub_a_concentrating.drop(columns=['timestamps','Right AUX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_a_relaxed' shape: (15192, 4)\n",
      "sub_a_neutral' shape: (15192, 4)\n",
      "sub_a_concentrating' shape: (15192, 4)\n"
     ]
    }
   ],
   "source": [
    "sub_a_relaxed = sub_a_relaxed[:15192]\n",
    "sub_a_neutral = sub_a_neutral[:15192]\n",
    "\n",
    "print('sub_a_relaxed\\' shape: {}'.format(sub_a_relaxed.shape))\n",
    "print('sub_a_neutral\\' shape: {}'.format(sub_a_neutral.shape))\n",
    "print('sub_a_concentrating\\' shape: {}'.format(sub_a_concentrating.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 15192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 30.762,  26.367,  21.484, ...,  21.484,  20.996,  33.203],\n",
       "        [ 15.625,  13.672,  14.16 , ...,  22.949,  21.973,  22.461],\n",
       "        [ 29.785,  28.32 ,  28.32 , ...,  34.18 ,  33.691,  31.738],\n",
       "        [  0.977,   0.   ,   2.93 , ...,  11.23 ,   5.859,  10.254]],\n",
       "\n",
       "       [[  4.883,  19.531,  20.508, ...,  34.668,  35.156,  20.02 ],\n",
       "        [ 22.949,  22.461,  19.531, ...,  33.203,  32.715,  30.273],\n",
       "        [  8.789,   5.371,  11.23 , ...,  16.602,  18.555,  10.254],\n",
       "        [ 23.926,  28.32 ,  25.391, ...,  25.391,  35.156,  18.555]],\n",
       "\n",
       "       [[ 59.105,  62.012,  44.922, ...,  48.828,  50.293,  45.41 ],\n",
       "        [ 28.32 ,  30.273,  30.273, ...,  31.25 ,  31.25 ,  30.273],\n",
       "        [ 15.137,  43.945, -97.656, ..., 258.789, 264.16 ,  27.344],\n",
       "        [ 12.207,  11.719,  11.23 , ...,  38.086,  39.551,  39.062]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([np.transpose(sub_a_relaxed), np.transpose(sub_a_neutral), np.transpose(sub_a_concentrating)])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label\n",
    "- 0.0 = relaxed<br>\n",
    "- 1.0 = neutral<br>\n",
    "- 2.0 = concetrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([0.0, 1.0, 2.0])\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction - Denoising\n",
    "Fast Fourier Transform (FFT) - A method to remove the artifact from the recorded data, such as eye blinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  6.57721652e-05  1.31544330e-04 ... -1.97316496e-04\n",
      " -1.31544330e-04 -6.57721652e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2d8a5c83d00>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy.fft as fft\n",
    "n = sub_a_relaxed.shape[0]\n",
    "spectrum = fft.fft(sub_a_relaxed['TP9'], n)\n",
    "freq = fft.fftfreq(len(spectrum))\n",
    "print(freq)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(freq, abs(spectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sub_a_relaxed.shape[0]\n",
    "fhat = fft.fft(sub_a_relaxed['TP9'], n)\n",
    "freq = fhat * np.conj(fhat) / n\n",
    "L = np.arrange(1, np.floor(n/2), dtype='int')\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.plot(freq, abs(spectrum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.31906614785993"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_per_freq = len(freq) / (257 / 2)\n",
    "points_per_freq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - EEGNet (Proven to be effective in classifying EEG Data)\n",
    "Parameter settings so far: nb_classes=3, as there are 3 classes to classify: relaxed, neutral, concentrating<br>\n",
    "\n",
    "Paper - https://arxiv.org/pdf/1611.08024.pdf <br>\n",
    "Github - https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Demo with 15192 samples (1 min period of mental state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 4, 15192, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 15192, 8)       60768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 15192, 8)       32        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 1, 15192, 16)      64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1, 15192, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1, 15192, 16)      0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 1, 3798, 16)       512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1, 3798, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 1, 474, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 474, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7584)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 22755     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 84,259\n",
      "Trainable params: 84,179\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "model  = EEGNet(nb_classes=3, Chans=4, Samples=X.shape[2],\n",
    "                dropoutRate = 0.5, kernLength=int(X.shape[2]/2), \n",
    "                F1=8, D=2, F2=16, dropoutType = 'Dropout')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 15192, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 9s 9s/step - loss: 1.2772 - accuracy: 0.6667\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.7128 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.2945 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.1381 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0612 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc216efdc0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_b_relaxed shape: (15192, 4)\n"
     ]
    }
   ],
   "source": [
    "# Using as a test dataset\n",
    "sub_b_relaxed = pd.read_csv(os.getcwd() + '/dataset/original_data/subjectb-relaxed-1.csv')\n",
    "sub_b_relaxed = sub_b_relaxed.drop(columns=['timestamps','Right AUX'])\n",
    "sub_b_relaxed = sub_b_relaxed[:15192]\n",
    "print('sub_b_relaxed shape: {}'.format(sub_b_relaxed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 15192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 43.457,  57.617,  41.504, ...,  17.09 ,  21.973,  47.363],\n",
       "        [ 39.551,  40.039,  41.016, ...,  22.949,  30.762,  35.156],\n",
       "        [ 48.828,  48.34 ,  47.852, ...,  22.949,  20.508,  27.832],\n",
       "        [ 19.043,  23.438,  25.879, ..., -19.043, -20.508, -13.672]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([np.transpose(sub_b_relaxed)])\n",
    "print(X_test.shape)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 15192, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.array([0])\n",
    "print(Y_test.shape)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np_utils.to_categorical(Y_test, num_classes=3)\n",
    "print(Y_test.shape)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10493065, 0.13374467, 0.7613246 ]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(X_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = probs.argmax(axis = -1)  \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.000000 \n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('suba-1_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Demo\n",
    "Using the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, chans, samples = 1, 4, 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4, 888, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 4, 888, 8)         3552      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 888, 8)         32        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 1, 888, 16)        64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1, 888, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 888, 16)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d (SeparableC (None, 1, 222, 16)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 222, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 27, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 27, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 432)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1299      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 5,587\n",
      "Trainable params: 5,507\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "model  = EEGNet(nb_classes=3, Chans=chans, Samples=samples,\n",
    "                dropoutRate = 0.5, kernLength=int(samples/2), \n",
    "                F1=8, D=2, F2=16, dropoutType = 'Dropout')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (18, 4, 888, 1)\n",
      "X_test.shape = (3, 4, 888, 1)\n",
      "X_val.shape = (3, 4, 888, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:18]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "print('X_train.shape = {}'.format(X_train.shape))\n",
    "\n",
    "X_test = data[18:21]\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "print('X_test.shape = {}'.format(X_test.shape))\n",
    "\n",
    "X_val = data[21:24]\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "print('X_val.shape = {}'.format(X_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 2, 2, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0, 1,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train.shape = (18, 3)\n",
      "Y_test.shape = (3, 3)\n",
      "Y_val.shape = (3, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import utils as np_utils\n",
    "Y_train = labels[:18]\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes=3)\n",
    "print('Y_train.shape = {}'.format(Y_train.shape))\n",
    "\n",
    "\n",
    "Y_test = labels[18:21]\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes=3)\n",
    "print('Y_test.shape = {}'.format(Y_test.shape))\n",
    "\n",
    "Y_val = labels[21:24]\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes=3)\n",
    "print('Y_val.shape = {}'.format(Y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 4s - loss: 1.6711 - accuracy: 0.4444 - val_loss: 1.1448 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 1.0666 - accuracy: 0.3333 - val_loss: 1.1340 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 1.0026 - accuracy: 0.5556 - val_loss: 1.1328 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 0.9637 - accuracy: 0.5000 - val_loss: 1.1355 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 0.9671 - accuracy: 0.4444 - val_loss: 1.1349 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 0.9488 - accuracy: 0.6111 - val_loss: 1.1358 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 0.9182 - accuracy: 0.6667 - val_loss: 1.1316 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 0.8342 - accuracy: 0.7222 - val_loss: 1.1272 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 0.8892 - accuracy: 0.6667 - val_loss: 1.1221 - val_accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 0.8694 - accuracy: 0.6667 - val_loss: 1.1178 - val_accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 0.8162 - accuracy: 0.7222 - val_loss: 1.1168 - val_accuracy: 0.3333\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 0.8312 - accuracy: 0.6667 - val_loss: 1.1131 - val_accuracy: 0.3333\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 0.8291 - accuracy: 0.6667 - val_loss: 1.1085 - val_accuracy: 0.3333\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 0.8729 - accuracy: 0.6111 - val_loss: 1.1076 - val_accuracy: 0.3333\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 0.8102 - accuracy: 0.7222 - val_loss: 1.1140 - val_accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 0.8024 - accuracy: 0.7222 - val_loss: 1.1289 - val_accuracy: 0.3333\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 0.7779 - accuracy: 0.7222 - val_loss: 1.1459 - val_accuracy: 0.3333\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 0.7938 - accuracy: 0.6667 - val_loss: 1.1596 - val_accuracy: 0.3333\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 0.7551 - accuracy: 0.6667 - val_loss: 1.1724 - val_accuracy: 0.3333\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 0.7754 - accuracy: 0.6111 - val_loss: 1.1791 - val_accuracy: 0.3333\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 0.7373 - accuracy: 0.7778 - val_loss: 1.1815 - val_accuracy: 0.3333\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 0.7468 - accuracy: 0.7222 - val_loss: 1.1814 - val_accuracy: 0.3333\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 0.7325 - accuracy: 0.8333 - val_loss: 1.1775 - val_accuracy: 0.3333\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 0.7109 - accuracy: 0.7778 - val_loss: 1.1686 - val_accuracy: 0.3333\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 0.7274 - accuracy: 0.7222 - val_loss: 1.1570 - val_accuracy: 0.3333\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 0.6969 - accuracy: 0.7778 - val_loss: 1.1455 - val_accuracy: 0.3333\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 0.6532 - accuracy: 0.8333 - val_loss: 1.1326 - val_accuracy: 0.3333\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 0.6923 - accuracy: 0.9444 - val_loss: 1.1258 - val_accuracy: 0.3333\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 0.6844 - accuracy: 0.8333 - val_loss: 1.1208 - val_accuracy: 0.3333\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 0.6480 - accuracy: 0.9444 - val_loss: 1.1165 - val_accuracy: 0.3333\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 0.6632 - accuracy: 0.8333 - val_loss: 1.1139 - val_accuracy: 0.3333\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 0.6136 - accuracy: 0.8889 - val_loss: 1.1116 - val_accuracy: 0.3333\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 0.6386 - accuracy: 0.9444 - val_loss: 1.1115 - val_accuracy: 0.3333\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 0.6267 - accuracy: 1.0000 - val_loss: 1.1148 - val_accuracy: 0.3333\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 0.6135 - accuracy: 0.9444 - val_loss: 1.1172 - val_accuracy: 0.3333\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 0.5829 - accuracy: 0.9444 - val_loss: 1.1172 - val_accuracy: 0.3333\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 0.5730 - accuracy: 0.9444 - val_loss: 1.1164 - val_accuracy: 0.3333\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 0.5635 - accuracy: 0.9444 - val_loss: 1.1123 - val_accuracy: 0.3333\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 0.5791 - accuracy: 0.8333 - val_loss: 1.1026 - val_accuracy: 0.3333\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 0.6288 - accuracy: 0.8889 - val_loss: 1.0913 - val_accuracy: 0.3333\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 0.5536 - accuracy: 0.7778 - val_loss: 1.0860 - val_accuracy: 0.3333\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 0.5360 - accuracy: 0.8333 - val_loss: 1.0863 - val_accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 0.5699 - accuracy: 0.8889 - val_loss: 1.0880 - val_accuracy: 0.3333\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 0.5381 - accuracy: 0.8333 - val_loss: 1.0931 - val_accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 0.6021 - accuracy: 0.8333 - val_loss: 1.0918 - val_accuracy: 0.3333\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 0.5928 - accuracy: 0.8889 - val_loss: 1.0762 - val_accuracy: 0.3333\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 0.5671 - accuracy: 0.8333 - val_loss: 1.0573 - val_accuracy: 0.6667\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 0.5342 - accuracy: 0.8333 - val_loss: 1.0437 - val_accuracy: 0.6667\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 0.5959 - accuracy: 0.8333 - val_loss: 1.0404 - val_accuracy: 0.6667\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 0.5368 - accuracy: 0.8333 - val_loss: 1.0432 - val_accuracy: 0.6667\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 0.5445 - accuracy: 0.8889 - val_loss: 1.0495 - val_accuracy: 0.6667\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 0.5185 - accuracy: 0.8889 - val_loss: 1.0596 - val_accuracy: 0.3333\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 0.5379 - accuracy: 0.8333 - val_loss: 1.0749 - val_accuracy: 0.3333\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 0.5160 - accuracy: 0.8333 - val_loss: 1.0906 - val_accuracy: 0.3333\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 0.5085 - accuracy: 0.9444 - val_loss: 1.1027 - val_accuracy: 0.3333\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 0.5102 - accuracy: 0.8333 - val_loss: 1.1112 - val_accuracy: 0.3333\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 0.5443 - accuracy: 0.8889 - val_loss: 1.1136 - val_accuracy: 0.3333\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 0.5025 - accuracy: 0.9444 - val_loss: 1.1150 - val_accuracy: 0.3333\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 0.5153 - accuracy: 0.8333 - val_loss: 1.1154 - val_accuracy: 0.3333\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 0.5137 - accuracy: 0.8889 - val_loss: 1.1168 - val_accuracy: 0.3333\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 0.5533 - accuracy: 0.7222 - val_loss: 1.1172 - val_accuracy: 0.3333\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 0.4619 - accuracy: 0.7778 - val_loss: 1.1136 - val_accuracy: 0.3333\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 0.4734 - accuracy: 0.8889 - val_loss: 1.1129 - val_accuracy: 0.3333\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 0.4539 - accuracy: 0.8333 - val_loss: 1.1214 - val_accuracy: 0.3333\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 0.4528 - accuracy: 0.8889 - val_loss: 1.1347 - val_accuracy: 0.3333\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 0.4873 - accuracy: 0.8889 - val_loss: 1.1498 - val_accuracy: 0.3333\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 0.4473 - accuracy: 0.8889 - val_loss: 1.1565 - val_accuracy: 0.3333\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 0.4566 - accuracy: 0.9444 - val_loss: 1.1610 - val_accuracy: 0.3333\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 0.4538 - accuracy: 0.9444 - val_loss: 1.1577 - val_accuracy: 0.3333\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 0.4810 - accuracy: 0.8889 - val_loss: 1.1496 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 0.3839 - accuracy: 1.0000 - val_loss: 1.1436 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 0.4027 - accuracy: 1.0000 - val_loss: 1.1368 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 0.4669 - accuracy: 0.9444 - val_loss: 1.1303 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 0.4285 - accuracy: 1.0000 - val_loss: 1.1227 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 0.4263 - accuracy: 1.0000 - val_loss: 1.1125 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 0.4255 - accuracy: 1.0000 - val_loss: 1.1014 - val_accuracy: 0.3333\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 0.4376 - accuracy: 0.9444 - val_loss: 1.0949 - val_accuracy: 0.3333\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 0.4053 - accuracy: 0.9444 - val_loss: 1.0951 - val_accuracy: 0.3333\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 0.4244 - accuracy: 0.8889 - val_loss: 1.0967 - val_accuracy: 0.3333\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 0.4123 - accuracy: 0.9444 - val_loss: 1.1064 - val_accuracy: 0.3333\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 0.4018 - accuracy: 0.9444 - val_loss: 1.1174 - val_accuracy: 0.3333\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 0.3616 - accuracy: 1.0000 - val_loss: 1.1293 - val_accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 0.3844 - accuracy: 0.8889 - val_loss: 1.1430 - val_accuracy: 0.3333\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 0.3849 - accuracy: 1.0000 - val_loss: 1.1470 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 0.3467 - accuracy: 1.0000 - val_loss: 1.1475 - val_accuracy: 0.3333\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 0.3438 - accuracy: 1.0000 - val_loss: 1.1473 - val_accuracy: 0.3333\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 0.3564 - accuracy: 1.0000 - val_loss: 1.1415 - val_accuracy: 0.3333\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 0.3406 - accuracy: 1.0000 - val_loss: 1.1252 - val_accuracy: 0.3333\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 0.3453 - accuracy: 1.0000 - val_loss: 1.1208 - val_accuracy: 0.3333\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 0.3502 - accuracy: 1.0000 - val_loss: 1.1319 - val_accuracy: 0.3333\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 0.3779 - accuracy: 1.0000 - val_loss: 1.1503 - val_accuracy: 0.3333\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 0.3296 - accuracy: 1.0000 - val_loss: 1.1588 - val_accuracy: 0.3333\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 0.3706 - accuracy: 0.9444 - val_loss: 1.1587 - val_accuracy: 0.3333\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 0.3619 - accuracy: 1.0000 - val_loss: 1.1377 - val_accuracy: 0.3333\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 0.3594 - accuracy: 1.0000 - val_loss: 1.1141 - val_accuracy: 0.3333\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 0.3536 - accuracy: 1.0000 - val_loss: 1.1089 - val_accuracy: 0.3333\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 0.3679 - accuracy: 0.8889 - val_loss: 1.1128 - val_accuracy: 0.3333\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 0.3530 - accuracy: 0.9444 - val_loss: 1.1297 - val_accuracy: 0.3333\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 0.3546 - accuracy: 0.9444 - val_loss: 1.1548 - val_accuracy: 0.3333\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 0.3673 - accuracy: 1.0000 - val_loss: 1.1794 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d8a5091e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 16, epochs=100,\n",
    "         verbose = 2, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.333333 \n"
     ]
    }
   ],
   "source": [
    "probs       = model.predict(X_val)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == Y_val.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Demo\n",
    "Using the feature extracted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mental_state shape: (2479, 989)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.781648</td>\n",
       "      <td>33.836367</td>\n",
       "      <td>-92.769629</td>\n",
       "      <td>19.187957</td>\n",
       "      <td>-1.542262</td>\n",
       "      <td>0.197462</td>\n",
       "      <td>-119.561133</td>\n",
       "      <td>2.032654</td>\n",
       "      <td>21.596272</td>\n",
       "      <td>33.965587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.357891</td>\n",
       "      <td>26.792566</td>\n",
       "      <td>417.203910</td>\n",
       "      <td>19.472121</td>\n",
       "      <td>-38.797263</td>\n",
       "      <td>-16.897194</td>\n",
       "      <td>-29.368531</td>\n",
       "      <td>-9.055370</td>\n",
       "      <td>44.647424</td>\n",
       "      <td>40.893307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.451926</td>\n",
       "      <td>31.076434</td>\n",
       "      <td>72.231301</td>\n",
       "      <td>14.245938</td>\n",
       "      <td>-13.225057</td>\n",
       "      <td>-0.614138</td>\n",
       "      <td>-28.331698</td>\n",
       "      <td>-8.858742</td>\n",
       "      <td>31.450289</td>\n",
       "      <td>30.692883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.282184</td>\n",
       "      <td>19.985184</td>\n",
       "      <td>16.220094</td>\n",
       "      <td>39.787312</td>\n",
       "      <td>1.847866</td>\n",
       "      <td>0.670216</td>\n",
       "      <td>-1.820355</td>\n",
       "      <td>20.220724</td>\n",
       "      <td>21.404679</td>\n",
       "      <td>20.777411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.010546</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.431516</td>\n",
       "      <td>28.982168</td>\n",
       "      <td>27.540246</td>\n",
       "      <td>19.960398</td>\n",
       "      <td>2.491458</td>\n",
       "      <td>-6.020503</td>\n",
       "      <td>-1.071166</td>\n",
       "      <td>2.655259</td>\n",
       "      <td>16.295039</td>\n",
       "      <td>32.658163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>15.762328</td>\n",
       "      <td>19.113555</td>\n",
       "      <td>23.696867</td>\n",
       "      <td>7.568395</td>\n",
       "      <td>-6.503336</td>\n",
       "      <td>6.867187</td>\n",
       "      <td>-11.955396</td>\n",
       "      <td>-16.519912</td>\n",
       "      <td>19.838319</td>\n",
       "      <td>14.333094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.008941</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>34.675582</td>\n",
       "      <td>34.200645</td>\n",
       "      <td>-57.624820</td>\n",
       "      <td>-4.825609</td>\n",
       "      <td>7.382353</td>\n",
       "      <td>2.324416</td>\n",
       "      <td>-1.341208</td>\n",
       "      <td>-4.178625</td>\n",
       "      <td>26.383597</td>\n",
       "      <td>28.782987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>29.813809</td>\n",
       "      <td>29.623031</td>\n",
       "      <td>-86.503988</td>\n",
       "      <td>7.532121</td>\n",
       "      <td>-19.581287</td>\n",
       "      <td>-0.628400</td>\n",
       "      <td>133.947160</td>\n",
       "      <td>-2.049096</td>\n",
       "      <td>45.484851</td>\n",
       "      <td>32.163999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>59.453973</td>\n",
       "      <td>17.944332</td>\n",
       "      <td>-10.164238</td>\n",
       "      <td>42.568211</td>\n",
       "      <td>-1.300655</td>\n",
       "      <td>-19.993690</td>\n",
       "      <td>-54.331696</td>\n",
       "      <td>12.947622</td>\n",
       "      <td>55.203380</td>\n",
       "      <td>40.228490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>22.893855</td>\n",
       "      <td>30.412723</td>\n",
       "      <td>26.029590</td>\n",
       "      <td>14.249789</td>\n",
       "      <td>-7.101478</td>\n",
       "      <td>-0.551013</td>\n",
       "      <td>3.735563</td>\n",
       "      <td>-9.372750</td>\n",
       "      <td>30.411574</td>\n",
       "      <td>30.079904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2479 rows × 989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "0       25.781648    33.836367   -92.769629    19.187957           -1.542262   \n",
       "1       29.357891    26.792566   417.203910    19.472121          -38.797263   \n",
       "2       28.451926    31.076434    72.231301    14.245938          -13.225057   \n",
       "3       21.282184    19.985184    16.220094    39.787312            1.847866   \n",
       "4       20.431516    28.982168    27.540246    19.960398            2.491458   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "2474    15.762328    19.113555    23.696867     7.568395           -6.503336   \n",
       "2475    34.675582    34.200645   -57.624820    -4.825609            7.382353   \n",
       "2476    29.813809    29.623031   -86.503988     7.532121          -19.581287   \n",
       "2477    59.453973    17.944332   -10.164238    42.568211           -1.300655   \n",
       "2478    22.893855    30.412723    26.029590    14.249789           -7.101478   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "0               0.197462         -119.561133            2.032654   \n",
       "1             -16.897194          -29.368531           -9.055370   \n",
       "2              -0.614138          -28.331698           -8.858742   \n",
       "3               0.670216           -1.820355           20.220724   \n",
       "4              -6.020503           -1.071166            2.655259   \n",
       "...                  ...                 ...                 ...   \n",
       "2474            6.867187          -11.955396          -16.519912   \n",
       "2475            2.324416           -1.341208           -4.178625   \n",
       "2476           -0.628400          133.947160           -2.049096   \n",
       "2477          -19.993690          -54.331696           12.947622   \n",
       "2478           -0.551013            3.735563           -9.372750   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_669_3  freq_679_3  freq_689_3  \\\n",
       "0          21.596272       33.965587  ...    0.000230    0.000351    0.000547   \n",
       "1          44.647424       40.893307  ...    0.001671    0.000740    0.001122   \n",
       "2          31.450289       30.692883  ...    0.000748    0.000569    0.000327   \n",
       "3          21.404679       20.777411  ...    0.000990    0.005644    0.006891   \n",
       "4          16.295039       32.658163  ...    0.001659    0.014379    0.014492   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "2474       19.838319       14.333094  ...    0.008537    0.008941    0.004102   \n",
       "2475       26.383597       28.782987  ...    0.003324    0.003593    0.001702   \n",
       "2476       45.484851       32.163999  ...    0.000754    0.000508    0.000263   \n",
       "2477       55.203380       40.228490  ...    0.003332    0.003557    0.004063   \n",
       "2478       30.411574       30.079904  ...    0.002470    0.003917    0.002528   \n",
       "\n",
       "      freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  freq_750_3  \\\n",
       "0       0.000381    0.000350    0.000453    0.000442    0.000325    0.000209   \n",
       "1       0.000521    0.000624    0.000439    0.001249    0.000727    0.000801   \n",
       "2       0.000197    0.000833    0.000909    0.000699    0.001165    0.000616   \n",
       "3       0.010546    0.009583    0.011158    0.008853    0.004551    0.002287   \n",
       "4       0.002949    0.004575    0.008305    0.007202    0.006957    0.009836   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2474    0.003156    0.003659    0.010179    0.004591    0.013817    0.004536   \n",
       "2475    0.003121    0.002686    0.001645    0.001770    0.001038    0.001973   \n",
       "2476    0.000701    0.000797    0.001096    0.000388    0.000529    0.001079   \n",
       "2477    0.001662    0.002665    0.002353    0.003976    0.001660    0.003229   \n",
       "2478    0.005357    0.004612    0.004503    0.003669    0.002316    0.004765   \n",
       "\n",
       "      Label  \n",
       "0       2.0  \n",
       "1       2.0  \n",
       "2       2.0  \n",
       "3       1.0  \n",
       "4       2.0  \n",
       "...     ...  \n",
       "2474    0.0  \n",
       "2475    2.0  \n",
       "2476    2.0  \n",
       "2477    2.0  \n",
       "2478    1.0  \n",
       "\n",
       "[2479 rows x 989 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mental_state = pd.read_csv(\"mental-state.csv\")\n",
    "\n",
    "print('mental_state shape: {}'.format(mental_state.shape))\n",
    "mental_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    830\n",
       "2.0    830\n",
       "0.0    819\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mental_state['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    y = df['Label'].copy()\n",
    "    X = df.drop('Label', axis=1).copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_inputs(mental_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1245)]            0         \n",
      "_________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)  (None, 1245, 1)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 1245, 256)         198912    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 318720)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 956163    \n",
      "=================================================================\n",
      "Total params: 1,155,075\n",
      "Trainable params: 1,155,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "expand_dims = tf.expand_dims(inputs, axis=2)\n",
    "\n",
    "gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(gru)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "\n",
    "model_989 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44/44 [==============================] - 197s 4s/step - loss: 5.3855 - accuracy: 0.6119 - val_loss: 0.8193 - val_accuracy: 0.8444\n",
      "Epoch 2/20\n",
      "44/44 [==============================] - 197s 4s/step - loss: 0.5102 - accuracy: 0.8887 - val_loss: 0.5470 - val_accuracy: 0.8905\n",
      "Epoch 3/20\n",
      "44/44 [==============================] - 198s 5s/step - loss: 0.1434 - accuracy: 0.9469 - val_loss: 0.4480 - val_accuracy: 0.9193\n",
      "Epoch 4/20\n",
      "44/44 [==============================] - 192s 4s/step - loss: 0.1646 - accuracy: 0.9464 - val_loss: 0.6178 - val_accuracy: 0.9078\n",
      "Epoch 5/20\n",
      "44/44 [==============================] - 196s 4s/step - loss: 0.0668 - accuracy: 0.9742 - val_loss: 0.4044 - val_accuracy: 0.9280\n",
      "Epoch 6/20\n",
      "44/44 [==============================] - 171s 4s/step - loss: 0.0332 - accuracy: 0.9860 - val_loss: 0.4706 - val_accuracy: 0.9164\n",
      "Epoch 7/20\n",
      "44/44 [==============================] - 157s 4s/step - loss: 0.0365 - accuracy: 0.9859 - val_loss: 0.6989 - val_accuracy: 0.8761\n",
      "Epoch 8/20\n",
      "44/44 [==============================] - 156s 4s/step - loss: 0.0453 - accuracy: 0.9836 - val_loss: 0.5862 - val_accuracy: 0.9164\n",
      "Epoch 9/20\n",
      "44/44 [==============================] - 156s 4s/step - loss: 0.0924 - accuracy: 0.9677 - val_loss: 0.5314 - val_accuracy: 0.9164\n",
      "Epoch 10/20\n",
      "44/44 [==============================] - 157s 4s/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 0.4889 - val_accuracy: 0.9308\n"
     ]
    }
   ],
   "source": [
    "model_989.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model_989.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.758%\n"
     ]
    }
   ],
   "source": [
    "model_acc = model_989.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Test Accuracy: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'RELAXED': 0, 'NEUTRAL': 1, 'CONCENTRATING': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_659_3</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>21.533953</td>\n",
       "      <td>24.349250</td>\n",
       "      <td>14.263184</td>\n",
       "      <td>19.638066</td>\n",
       "      <td>-1.596033</td>\n",
       "      <td>-2.303877</td>\n",
       "      <td>7.172422</td>\n",
       "      <td>-2.758178</td>\n",
       "      <td>27.777065</td>\n",
       "      <td>27.413900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.007981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>13.854969</td>\n",
       "      <td>30.963898</td>\n",
       "      <td>86.671848</td>\n",
       "      <td>10.726926</td>\n",
       "      <td>11.014309</td>\n",
       "      <td>-2.351598</td>\n",
       "      <td>-96.565275</td>\n",
       "      <td>5.507241</td>\n",
       "      <td>7.679991</td>\n",
       "      <td>35.161736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.010142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>92.313766</td>\n",
       "      <td>-27.812984</td>\n",
       "      <td>-33.092516</td>\n",
       "      <td>69.694520</td>\n",
       "      <td>37.149972</td>\n",
       "      <td>70.093665</td>\n",
       "      <td>257.544227</td>\n",
       "      <td>24.399168</td>\n",
       "      <td>90.861099</td>\n",
       "      <td>-49.892207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>20.242703</td>\n",
       "      <td>33.720023</td>\n",
       "      <td>33.735270</td>\n",
       "      <td>24.385453</td>\n",
       "      <td>2.376185</td>\n",
       "      <td>66.385647</td>\n",
       "      <td>9.040695</td>\n",
       "      <td>-9.018088</td>\n",
       "      <td>11.874386</td>\n",
       "      <td>-8.302259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.003821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>36.272039</td>\n",
       "      <td>27.124422</td>\n",
       "      <td>-33.611277</td>\n",
       "      <td>21.467242</td>\n",
       "      <td>-2.740810</td>\n",
       "      <td>1.210147</td>\n",
       "      <td>-2.092335</td>\n",
       "      <td>-5.686588</td>\n",
       "      <td>36.508013</td>\n",
       "      <td>25.270959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>38.717262</td>\n",
       "      <td>35.903930</td>\n",
       "      <td>30.488977</td>\n",
       "      <td>38.679148</td>\n",
       "      <td>-6.559766</td>\n",
       "      <td>-3.534145</td>\n",
       "      <td>-23.642179</td>\n",
       "      <td>-6.966519</td>\n",
       "      <td>35.499137</td>\n",
       "      <td>36.703360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>53.167348</td>\n",
       "      <td>29.899617</td>\n",
       "      <td>-53.899781</td>\n",
       "      <td>37.067426</td>\n",
       "      <td>-31.999664</td>\n",
       "      <td>-6.244550</td>\n",
       "      <td>-10.205804</td>\n",
       "      <td>-30.779264</td>\n",
       "      <td>93.524951</td>\n",
       "      <td>37.682383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>27.872078</td>\n",
       "      <td>27.441035</td>\n",
       "      <td>30.076984</td>\n",
       "      <td>23.603434</td>\n",
       "      <td>1.334606</td>\n",
       "      <td>-4.799774</td>\n",
       "      <td>30.758425</td>\n",
       "      <td>11.939336</td>\n",
       "      <td>30.805420</td>\n",
       "      <td>28.823100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>0.018422</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.012838</td>\n",
       "      <td>0.014391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23.331109</td>\n",
       "      <td>27.687673</td>\n",
       "      <td>86.342198</td>\n",
       "      <td>20.713019</td>\n",
       "      <td>-3.626738</td>\n",
       "      <td>3.980956</td>\n",
       "      <td>102.890579</td>\n",
       "      <td>2.351927</td>\n",
       "      <td>28.806945</td>\n",
       "      <td>24.416551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>21.419508</td>\n",
       "      <td>18.131242</td>\n",
       "      <td>15.830980</td>\n",
       "      <td>-2.193449</td>\n",
       "      <td>-0.977177</td>\n",
       "      <td>4.337088</td>\n",
       "      <td>-1.521976</td>\n",
       "      <td>17.445033</td>\n",
       "      <td>23.709483</td>\n",
       "      <td>19.733010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.010156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 988 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "2412    21.533953    24.349250    14.263184    19.638066           -1.596033   \n",
       "792     13.854969    30.963898    86.671848    10.726926           11.014309   \n",
       "564     92.313766   -27.812984   -33.092516    69.694520           37.149972   \n",
       "1728    20.242703    33.720023    33.735270    24.385453            2.376185   \n",
       "1034    36.272039    27.124422   -33.611277    21.467242           -2.740810   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "1935    38.717262    35.903930    30.488977    38.679148           -6.559766   \n",
       "1059    53.167348    29.899617   -53.899781    37.067426          -31.999664   \n",
       "2434    27.872078    27.441035    30.076984    23.603434            1.334606   \n",
       "15      23.331109    27.687673    86.342198    20.713019           -3.626738   \n",
       "1204    21.419508    18.131242    15.830980    -2.193449           -0.977177   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "2412           -2.303877            7.172422           -2.758178   \n",
       "792            -2.351598          -96.565275            5.507241   \n",
       "564            70.093665          257.544227           24.399168   \n",
       "1728           66.385647            9.040695           -9.018088   \n",
       "1034            1.210147           -2.092335           -5.686588   \n",
       "...                  ...                 ...                 ...   \n",
       "1935           -3.534145          -23.642179           -6.966519   \n",
       "1059           -6.244550          -10.205804          -30.779264   \n",
       "2434           -4.799774           30.758425           11.939336   \n",
       "15              3.980956          102.890579            2.351927   \n",
       "1204            4.337088           -1.521976           17.445033   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_659_3  freq_669_3  freq_679_3  \\\n",
       "2412       27.777065       27.413900  ...    0.006086    0.007197    0.012766   \n",
       "792         7.679991       35.161736  ...    0.013451    0.005752    0.010372   \n",
       "564        90.861099      -49.892207  ...    0.000166    0.000828    0.002567   \n",
       "1728       11.874386       -8.302259  ...    0.003197    0.001923    0.000472   \n",
       "1034       36.508013       25.270959  ...    0.002429    0.001517    0.000897   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "1935       35.499137       36.703360  ...    0.009700    0.007417    0.009890   \n",
       "1059       93.524951       37.682383  ...    0.000721    0.002283    0.000670   \n",
       "2434       30.805420       28.823100  ...    0.006328    0.010904    0.018422   \n",
       "15         28.806945       24.416551  ...    0.000572    0.000563    0.000393   \n",
       "1204       23.709483       19.733010  ...    0.008214    0.015572    0.016297   \n",
       "\n",
       "      freq_689_3  freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  \\\n",
       "2412    0.008290    0.011961    0.004939    0.022888    0.006256    0.008407   \n",
       "792     0.001696    0.017567    0.010390    0.005568    0.005870    0.003657   \n",
       "564     0.000455    0.001419    0.004096    0.002097    0.001676    0.001924   \n",
       "1728    0.003554    0.004350    0.001391    0.003987    0.001525    0.004844   \n",
       "1034    0.000466    0.000372    0.001456    0.000588    0.000391    0.000921   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1935    0.007237    0.005449    0.002260    0.002470    0.006113    0.006091   \n",
       "1059    0.001303    0.000687    0.000899    0.001852    0.000705    0.001225   \n",
       "2434    0.002817    0.006737    0.018157    0.007096    0.001726    0.012838   \n",
       "15      0.001054    0.000611    0.000548    0.000535    0.000593    0.000752   \n",
       "1204    0.012703    0.002941    0.005034    0.003930    0.009099    0.004269   \n",
       "\n",
       "      freq_750_3  \n",
       "2412    0.007981  \n",
       "792     0.010142  \n",
       "564     0.000989  \n",
       "1728    0.003821  \n",
       "1034    0.000721  \n",
       "...          ...  \n",
       "1935    0.003284  \n",
       "1059    0.000572  \n",
       "2434    0.014391  \n",
       "15      0.000922  \n",
       "1204    0.010156  \n",
       "\n",
       "[744 rows x 988 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2412    0.0\n",
       "792     1.0\n",
       "564     2.0\n",
       "1728    2.0\n",
       "1034    2.0\n",
       "       ... \n",
       "1935    2.0\n",
       "1059    2.0\n",
       "2434    1.0\n",
       "15      1.0\n",
       "1204    1.0\n",
       "Name: Label, Length: 744, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9963522e-01, 3.6465641e-04, 1.6611828e-07],\n",
       "       [1.9576350e-07, 9.9999917e-01, 6.0598194e-07],\n",
       "       [4.1574369e-10, 1.7011987e-09, 1.0000000e+00],\n",
       "       ...,\n",
       "       [1.1131902e-05, 9.9889958e-01, 1.0892919e-03],\n",
       "       [4.8530450e-11, 9.7265673e-01, 2.7343281e-02],\n",
       "       [7.3799286e-03, 9.9261403e-01, 6.0753728e-06]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_989.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      RELAXED       0.92      0.96      0.94       244\n",
      "      NEUTRAL       0.94      0.93      0.93       269\n",
      "CONCENTRATING       0.98      0.96      0.97       231\n",
      "\n",
      "     accuracy                           0.95       744\n",
      "    macro avg       0.95      0.95      0.95       744\n",
      " weighted avg       0.95      0.95      0.95       744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = np.array(list(map(lambda x: np.argmax(x), model_989.predict(X_test))))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clr = classification_report(y_test, y_pred, target_names=label_mapping.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.yticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_989.save('989_gru.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new csv from github dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14056\\Desktop\\EEG_Classification/dataset/original_data\n",
      "name-concentrating-1\n",
      "name\n",
      "concentrating\n",
      "Using file name-concentrating-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjecta-concentrating-1\n",
      "subjecta\n",
      "concentrating\n",
      "Using file subjecta-concentrating-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjecta-concentrating-2\n",
      "subjecta\n",
      "concentrating\n",
      "Using file subjecta-concentrating-2.csv\n",
      "resulting vector shape for the file (102, 1246)\n",
      "subjecta-neutral-1\n",
      "subjecta\n",
      "neutral\n",
      "Using file subjecta-neutral-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjecta-neutral-2\n",
      "subjecta\n",
      "neutral\n",
      "Using file subjecta-neutral-2.csv\n",
      "resulting vector shape for the file (117, 1246)\n",
      "subjecta-relaxed-1\n",
      "subjecta\n",
      "relaxed\n",
      "Using file subjecta-relaxed-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjecta-relaxed-2\n",
      "subjecta\n",
      "relaxed\n",
      "Using file subjecta-relaxed-2.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectb-concentrating-1\n",
      "subjectb\n",
      "concentrating\n",
      "Using file subjectb-concentrating-1.csv\n",
      "resulting vector shape for the file (86, 1246)\n",
      "subjectb-concentrating-2\n",
      "subjectb\n",
      "concentrating\n",
      "Using file subjectb-concentrating-2.csv\n",
      "resulting vector shape for the file (87, 1246)\n",
      "subjectb-neutral-1\n",
      "subjectb\n",
      "neutral\n",
      "Using file subjectb-neutral-1.csv\n",
      "resulting vector shape for the file (117, 1246)\n",
      "subjectb-neutral-2\n",
      "subjectb\n",
      "neutral\n",
      "Using file subjectb-neutral-2.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectb-relaxed-1\n",
      "subjectb\n",
      "relaxed\n",
      "Using file subjectb-relaxed-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectb-relaxed-2\n",
      "subjectb\n",
      "relaxed\n",
      "Using file subjectb-relaxed-2.csv\n",
      "resulting vector shape for the file (6, 1246)\n",
      "subjectc-concentrating-1\n",
      "subjectc\n",
      "concentrating\n",
      "Using file subjectc-concentrating-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectc-concentrating-2\n",
      "subjectc\n",
      "concentrating\n",
      "Using file subjectc-concentrating-2.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectc-neutral-1\n",
      "subjectc\n",
      "neutral\n",
      "Using file subjectc-neutral-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectc-neutral-2\n",
      "subjectc\n",
      "neutral\n",
      "Using file subjectc-neutral-2.csv\n",
      "resulting vector shape for the file (16, 1246)\n",
      "subjectc-relaxed-1\n",
      "subjectc\n",
      "relaxed\n",
      "Using file subjectc-relaxed-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectc-relaxed-2\n",
      "subjectc\n",
      "relaxed\n",
      "Using file subjectc-relaxed-2.csv\n",
      "resulting vector shape for the file (117, 1246)\n",
      "subjectd-concentrating-1\n",
      "subjectd\n",
      "concentrating\n",
      "Using file subjectd-concentrating-1.csv\n",
      "resulting vector shape for the file (86, 1246)\n",
      "subjectd-concentrating-2\n",
      "subjectd\n",
      "concentrating\n",
      "Using file subjectd-concentrating-2.csv\n",
      "resulting vector shape for the file (5, 1246)\n",
      "subjectd-neutral-1\n",
      "subjectd\n",
      "neutral\n",
      "Using file subjectd-neutral-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectd-neutral-2\n",
      "subjectd\n",
      "neutral\n",
      "Using file subjectd-neutral-2.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectd-relaxed-1\n",
      "subjectd\n",
      "relaxed\n",
      "Using file subjectd-relaxed-1.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "subjectd-relaxed-2\n",
      "subjectd\n",
      "relaxed\n",
      "Using file subjectd-relaxed-2.csv\n",
      "resulting vector shape for the file (116, 1246)\n",
      "FINAL_MATRIX (2479, 1246)\n"
     ]
    }
   ],
   "source": [
    "import EEG_feature_extraction as efe\n",
    "from EEG_generate_training_matrix import gen_training_matrix\n",
    "\n",
    "path = os.getcwd() + '/dataset/original_data'\n",
    "print(path)\n",
    "mat = gen_training_matrix(path, 'mind_wandering', [])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2479, 1246)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_d_h2h1_4</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_4</th>\n",
       "      <th>freq_679_4</th>\n",
       "      <th>freq_689_4</th>\n",
       "      <th>freq_699_4</th>\n",
       "      <th>freq_709_4</th>\n",
       "      <th>freq_720_4</th>\n",
       "      <th>freq_730_4</th>\n",
       "      <th>freq_740_4</th>\n",
       "      <th>freq_750_4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.481383</td>\n",
       "      <td>15.838645</td>\n",
       "      <td>20.923613</td>\n",
       "      <td>3.868094</td>\n",
       "      <td>17.375938</td>\n",
       "      <td>1.753072</td>\n",
       "      <td>2.773070</td>\n",
       "      <td>1.560500</td>\n",
       "      <td>15.067953</td>\n",
       "      <td>-2.449311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.031733</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.124876</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>0.060481</td>\n",
       "      <td>0.057522</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.733152</td>\n",
       "      <td>28.509188</td>\n",
       "      <td>27.360910</td>\n",
       "      <td>-9.843813</td>\n",
       "      <td>25.695789</td>\n",
       "      <td>0.575784</td>\n",
       "      <td>4.896850</td>\n",
       "      <td>-12.705741</td>\n",
       "      <td>3.139401</td>\n",
       "      <td>3.876334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>0.041627</td>\n",
       "      <td>0.038336</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.023462</td>\n",
       "      <td>0.047628</td>\n",
       "      <td>0.043392</td>\n",
       "      <td>0.064315</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.083816</td>\n",
       "      <td>19.895562</td>\n",
       "      <td>18.968566</td>\n",
       "      <td>20.132062</td>\n",
       "      <td>24.353063</td>\n",
       "      <td>8.458956</td>\n",
       "      <td>-6.960552</td>\n",
       "      <td>8.420081</td>\n",
       "      <td>13.663810</td>\n",
       "      <td>2.692684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>0.036274</td>\n",
       "      <td>0.035740</td>\n",
       "      <td>0.038103</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>0.015408</td>\n",
       "      <td>0.060786</td>\n",
       "      <td>0.064373</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.253930</td>\n",
       "      <td>25.844574</td>\n",
       "      <td>79.116809</td>\n",
       "      <td>8.342746</td>\n",
       "      <td>20.599352</td>\n",
       "      <td>-2.702003</td>\n",
       "      <td>-0.594606</td>\n",
       "      <td>-38.274286</td>\n",
       "      <td>-2.628310</td>\n",
       "      <td>-5.524813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>0.046494</td>\n",
       "      <td>0.057281</td>\n",
       "      <td>0.058323</td>\n",
       "      <td>0.043862</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.087944</td>\n",
       "      <td>0.013122</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.979984</td>\n",
       "      <td>35.343152</td>\n",
       "      <td>-129.270523</td>\n",
       "      <td>25.274320</td>\n",
       "      <td>21.945969</td>\n",
       "      <td>4.657734</td>\n",
       "      <td>2.249219</td>\n",
       "      <td>-69.488195</td>\n",
       "      <td>-0.203635</td>\n",
       "      <td>-0.735726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.005622</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>24.408328</td>\n",
       "      <td>25.667238</td>\n",
       "      <td>30.307758</td>\n",
       "      <td>24.299613</td>\n",
       "      <td>22.199598</td>\n",
       "      <td>0.946822</td>\n",
       "      <td>-2.385283</td>\n",
       "      <td>-5.065522</td>\n",
       "      <td>-2.195507</td>\n",
       "      <td>4.632400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060562</td>\n",
       "      <td>0.050219</td>\n",
       "      <td>0.028050</td>\n",
       "      <td>0.037710</td>\n",
       "      <td>0.058077</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>0.034661</td>\n",
       "      <td>0.073741</td>\n",
       "      <td>0.079413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>24.288141</td>\n",
       "      <td>21.467258</td>\n",
       "      <td>29.664977</td>\n",
       "      <td>12.277609</td>\n",
       "      <td>26.782973</td>\n",
       "      <td>-2.271717</td>\n",
       "      <td>2.137290</td>\n",
       "      <td>-2.571791</td>\n",
       "      <td>0.258334</td>\n",
       "      <td>3.943137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110801</td>\n",
       "      <td>0.048037</td>\n",
       "      <td>0.030653</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>0.046190</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>0.025646</td>\n",
       "      <td>0.032765</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>16.386016</td>\n",
       "      <td>25.449730</td>\n",
       "      <td>27.654664</td>\n",
       "      <td>10.263465</td>\n",
       "      <td>26.906937</td>\n",
       "      <td>-11.676210</td>\n",
       "      <td>3.265842</td>\n",
       "      <td>-1.749670</td>\n",
       "      <td>-15.211361</td>\n",
       "      <td>0.561848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.073301</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.035329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>22.987340</td>\n",
       "      <td>24.354957</td>\n",
       "      <td>24.288199</td>\n",
       "      <td>17.356859</td>\n",
       "      <td>24.240461</td>\n",
       "      <td>-0.552771</td>\n",
       "      <td>3.836906</td>\n",
       "      <td>37.554667</td>\n",
       "      <td>-1.576095</td>\n",
       "      <td>-4.530965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049001</td>\n",
       "      <td>0.083742</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.034685</td>\n",
       "      <td>0.037678</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>0.018358</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>0.042973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>22.590140</td>\n",
       "      <td>25.563514</td>\n",
       "      <td>159.958646</td>\n",
       "      <td>2.268533</td>\n",
       "      <td>26.490716</td>\n",
       "      <td>-32.764340</td>\n",
       "      <td>-5.290471</td>\n",
       "      <td>71.470028</td>\n",
       "      <td>12.029387</td>\n",
       "      <td>3.472544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2479 rows × 1246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "0       16.481383    15.838645    20.923613     3.868094    17.375938   \n",
       "1       23.733152    28.509188    27.360910    -9.843813    25.695789   \n",
       "2       21.083816    19.895562    18.968566    20.132062    24.353063   \n",
       "3        3.253930    25.844574    79.116809     8.342746    20.599352   \n",
       "4       32.979984    35.343152  -129.270523    25.274320    21.945969   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2474    24.408328    25.667238    30.307758    24.299613    22.199598   \n",
       "2475    24.288141    21.467258    29.664977    12.277609    26.782973   \n",
       "2476    16.386016    25.449730    27.654664    10.263465    26.906937   \n",
       "2477    22.987340    24.354957    24.288199    17.356859    24.240461   \n",
       "2478    22.590140    25.563514   159.958646     2.268533    26.490716   \n",
       "\n",
       "      lag1_mean_d_h2h1_0  lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  \\\n",
       "0               1.753072            2.773070            1.560500   \n",
       "1               0.575784            4.896850          -12.705741   \n",
       "2               8.458956           -6.960552            8.420081   \n",
       "3              -2.702003           -0.594606          -38.274286   \n",
       "4               4.657734            2.249219          -69.488195   \n",
       "...                  ...                 ...                 ...   \n",
       "2474            0.946822           -2.385283           -5.065522   \n",
       "2475           -2.271717            2.137290           -2.571791   \n",
       "2476          -11.676210            3.265842           -1.749670   \n",
       "2477           -0.552771            3.836906           37.554667   \n",
       "2478          -32.764340           -5.290471           71.470028   \n",
       "\n",
       "      lag1_mean_d_h2h1_3  lag1_mean_d_h2h1_4  ...  freq_669_4  freq_679_4  \\\n",
       "0              15.067953           -2.449311  ...    0.085100    0.064925   \n",
       "1               3.139401            3.876334  ...    0.030827    0.041627   \n",
       "2              13.663810            2.692684  ...    0.053886    0.036274   \n",
       "3              -2.628310           -5.524813  ...    0.036960    0.046494   \n",
       "4              -0.203635           -0.735726  ...    0.005689    0.005457   \n",
       "...                  ...                 ...  ...         ...         ...   \n",
       "2474           -2.195507            4.632400  ...    0.060562    0.050219   \n",
       "2475            0.258334            3.943137  ...    0.110801    0.048037   \n",
       "2476          -15.211361            0.561848  ...    0.060503    0.056769   \n",
       "2477           -1.576095           -4.530965  ...    0.049001    0.083742   \n",
       "2478           12.029387            3.472544  ...    0.006308    0.015513   \n",
       "\n",
       "      freq_689_4  freq_699_4  freq_709_4  freq_720_4  freq_730_4  freq_740_4  \\\n",
       "0       0.031733    0.050842    0.074206    0.124876    0.018076    0.060481   \n",
       "1       0.038336    0.013650    0.023462    0.047628    0.043392    0.064315   \n",
       "2       0.035740    0.038103    0.049851    0.015408    0.060786    0.064373   \n",
       "3       0.057281    0.058323    0.043862    0.053333    0.087944    0.013122   \n",
       "4       0.001655    0.005883    0.004656    0.004733    0.004073    0.005072   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2474    0.028050    0.037710    0.058077    0.046329    0.034661    0.073741   \n",
       "2475    0.030653    0.051616    0.046190    0.051400    0.040910    0.025646   \n",
       "2476    0.016124    0.029197    0.031900    0.073301    0.015888    0.002436   \n",
       "2477    0.012564    0.034685    0.037678    0.022842    0.018358    0.011174   \n",
       "2478    0.003943    0.006063    0.005056    0.001390    0.008252    0.008875   \n",
       "\n",
       "      freq_750_4  Label  \n",
       "0       0.057522    0.0  \n",
       "1       0.024526    0.0  \n",
       "2       0.020419    1.0  \n",
       "3       0.033914    0.0  \n",
       "4       0.005622    2.0  \n",
       "...          ...    ...  \n",
       "2474    0.079413    0.0  \n",
       "2475    0.032765    0.0  \n",
       "2476    0.035329    1.0  \n",
       "2477    0.042973    0.0  \n",
       "2478    0.003099    2.0  \n",
       "\n",
       "[2479 rows x 1246 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mind_wandering = pd.read_csv('mind_wandering')\n",
    "print(mind_wandering.shape)\n",
    "mind_wandering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    830\n",
       "1.0    830\n",
       "0.0    819\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mind_wandering['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    y = df['Label'].copy()\n",
    "    X = df.drop('Label', axis=1).copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(mind_wandering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1245)]            0         \n",
      "_________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)  (None, 1245, 1)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 1245, 256)         198912    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 318720)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 956163    \n",
      "=================================================================\n",
      "Total params: 1,155,075\n",
      "Trainable params: 1,155,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "expand_dims = tf.expand_dims(inputs, axis=2)\n",
    "\n",
    "gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(gru)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(flatten)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44/44 [==============================] - 190s 4s/step - loss: 6.0749 - accuracy: 0.5948 - val_loss: 1.0480 - val_accuracy: 0.8271\n",
      "Epoch 2/20\n",
      "44/44 [==============================] - 169s 4s/step - loss: 0.5377 - accuracy: 0.8867 - val_loss: 0.9000 - val_accuracy: 0.8415\n",
      "Epoch 3/20\n",
      "44/44 [==============================] - 172s 4s/step - loss: 0.2303 - accuracy: 0.9393 - val_loss: 0.4307 - val_accuracy: 0.9020\n",
      "Epoch 4/20\n",
      "44/44 [==============================] - 173s 4s/step - loss: 0.1589 - accuracy: 0.9595 - val_loss: 0.5964 - val_accuracy: 0.9020\n",
      "Epoch 5/20\n",
      "44/44 [==============================] - 173s 4s/step - loss: 0.0635 - accuracy: 0.9790 - val_loss: 0.3899 - val_accuracy: 0.9366\n",
      "Epoch 6/20\n",
      "44/44 [==============================] - 180s 4s/step - loss: 0.0191 - accuracy: 0.9935 - val_loss: 0.5941 - val_accuracy: 0.9395\n",
      "Epoch 7/20\n",
      "44/44 [==============================] - 206s 5s/step - loss: 0.1160 - accuracy: 0.9719 - val_loss: 0.4299 - val_accuracy: 0.9020\n",
      "Epoch 8/20\n",
      "44/44 [==============================] - 221s 5s/step - loss: 0.0814 - accuracy: 0.9708 - val_loss: 0.3379 - val_accuracy: 0.9452\n",
      "Epoch 9/20\n",
      "44/44 [==============================] - 215s 5s/step - loss: 0.0424 - accuracy: 0.9798 - val_loss: 0.4096 - val_accuracy: 0.9308\n",
      "Epoch 10/20\n",
      "44/44 [==============================] - 215s 5s/step - loss: 0.0129 - accuracy: 0.9949 - val_loss: 0.4538 - val_accuracy: 0.9337\n",
      "Epoch 11/20\n",
      "44/44 [==============================] - 192s 4s/step - loss: 0.0052 - accuracy: 0.9976 - val_loss: 0.4510 - val_accuracy: 0.9452\n",
      "Epoch 12/20\n",
      "44/44 [==============================] - 211s 5s/step - loss: 2.3384e-04 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9452\n",
      "Epoch 13/20\n",
      "44/44 [==============================] - 290s 7s/step - loss: 1.0899e-04 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9452\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.742%\n"
     ]
    }
   ],
   "source": [
    "model_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Test Accuracy: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'RELAXED': 0, 'NEUTRAL': 1, 'CONCENTRATING': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_d_h2h1_4</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_659_4</th>\n",
       "      <th>freq_669_4</th>\n",
       "      <th>freq_679_4</th>\n",
       "      <th>freq_689_4</th>\n",
       "      <th>freq_699_4</th>\n",
       "      <th>freq_709_4</th>\n",
       "      <th>freq_720_4</th>\n",
       "      <th>freq_730_4</th>\n",
       "      <th>freq_740_4</th>\n",
       "      <th>freq_750_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>25.424973</td>\n",
       "      <td>-17.614375</td>\n",
       "      <td>32.493598</td>\n",
       "      <td>21.865875</td>\n",
       "      <td>29.127141</td>\n",
       "      <td>-18.878153</td>\n",
       "      <td>0.489474</td>\n",
       "      <td>16.780905</td>\n",
       "      <td>-2.257252</td>\n",
       "      <td>-4.993806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.028514</td>\n",
       "      <td>0.034570</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.045188</td>\n",
       "      <td>0.016358</td>\n",
       "      <td>0.022625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>15.718484</td>\n",
       "      <td>115.821867</td>\n",
       "      <td>-579.490660</td>\n",
       "      <td>12.979508</td>\n",
       "      <td>24.660113</td>\n",
       "      <td>19.649232</td>\n",
       "      <td>-170.145955</td>\n",
       "      <td>720.924986</td>\n",
       "      <td>28.940793</td>\n",
       "      <td>-2.088506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.005463</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.004544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>30.364977</td>\n",
       "      <td>46.558379</td>\n",
       "      <td>-26.126867</td>\n",
       "      <td>17.023105</td>\n",
       "      <td>23.008359</td>\n",
       "      <td>14.767124</td>\n",
       "      <td>0.064933</td>\n",
       "      <td>37.776384</td>\n",
       "      <td>4.238803</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.022987</td>\n",
       "      <td>0.014199</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>27.496336</td>\n",
       "      <td>20.496406</td>\n",
       "      <td>31.045867</td>\n",
       "      <td>-3.162359</td>\n",
       "      <td>24.070746</td>\n",
       "      <td>-8.435205</td>\n",
       "      <td>2.915673</td>\n",
       "      <td>-3.596188</td>\n",
       "      <td>-13.659502</td>\n",
       "      <td>-2.054757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067564</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>0.067106</td>\n",
       "      <td>0.037596</td>\n",
       "      <td>0.029957</td>\n",
       "      <td>0.052189</td>\n",
       "      <td>0.070352</td>\n",
       "      <td>0.057723</td>\n",
       "      <td>0.006037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>19.062039</td>\n",
       "      <td>21.963160</td>\n",
       "      <td>32.930387</td>\n",
       "      <td>13.996117</td>\n",
       "      <td>27.780520</td>\n",
       "      <td>-16.266509</td>\n",
       "      <td>2.341476</td>\n",
       "      <td>8.174973</td>\n",
       "      <td>-6.269097</td>\n",
       "      <td>4.000325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040791</td>\n",
       "      <td>0.032998</td>\n",
       "      <td>0.035613</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.089029</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.087388</td>\n",
       "      <td>0.020152</td>\n",
       "      <td>0.050728</td>\n",
       "      <td>0.065582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>32.642367</td>\n",
       "      <td>29.186234</td>\n",
       "      <td>56.669238</td>\n",
       "      <td>22.665027</td>\n",
       "      <td>22.583023</td>\n",
       "      <td>59.786704</td>\n",
       "      <td>12.293798</td>\n",
       "      <td>-20.877694</td>\n",
       "      <td>59.814028</td>\n",
       "      <td>-3.487232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.009225</td>\n",
       "      <td>0.007511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>40.023809</td>\n",
       "      <td>35.621648</td>\n",
       "      <td>-62.879543</td>\n",
       "      <td>-4.518492</td>\n",
       "      <td>24.913797</td>\n",
       "      <td>3.340981</td>\n",
       "      <td>0.509492</td>\n",
       "      <td>-8.996983</td>\n",
       "      <td>4.844432</td>\n",
       "      <td>-1.288017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016422</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.024865</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.018285</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>0.020504</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.008336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>35.188715</td>\n",
       "      <td>32.291406</td>\n",
       "      <td>-69.272988</td>\n",
       "      <td>-0.820188</td>\n",
       "      <td>24.852734</td>\n",
       "      <td>-8.261413</td>\n",
       "      <td>-0.207049</td>\n",
       "      <td>9.248070</td>\n",
       "      <td>-1.986913</td>\n",
       "      <td>5.773653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022761</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.023723</td>\n",
       "      <td>0.036557</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.038976</td>\n",
       "      <td>0.009429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.370262</td>\n",
       "      <td>-0.471125</td>\n",
       "      <td>32.270395</td>\n",
       "      <td>9.672199</td>\n",
       "      <td>24.974848</td>\n",
       "      <td>2.655548</td>\n",
       "      <td>33.326193</td>\n",
       "      <td>-17.298875</td>\n",
       "      <td>-22.121268</td>\n",
       "      <td>-3.426928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034116</td>\n",
       "      <td>0.019738</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.023540</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.012127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>14.963180</td>\n",
       "      <td>20.307562</td>\n",
       "      <td>18.939926</td>\n",
       "      <td>13.221715</td>\n",
       "      <td>28.886793</td>\n",
       "      <td>-8.174696</td>\n",
       "      <td>-1.366989</td>\n",
       "      <td>0.052292</td>\n",
       "      <td>-12.047434</td>\n",
       "      <td>6.061144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045316</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>0.035277</td>\n",
       "      <td>0.022084</td>\n",
       "      <td>0.031079</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>0.041514</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.051987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 1245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "2412    25.424973   -17.614375    32.493598    21.865875    29.127141   \n",
       "792     15.718484   115.821867  -579.490660    12.979508    24.660113   \n",
       "564     30.364977    46.558379   -26.126867    17.023105    23.008359   \n",
       "1728    27.496336    20.496406    31.045867    -3.162359    24.070746   \n",
       "1034    19.062039    21.963160    32.930387    13.996117    27.780520   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1935    32.642367    29.186234    56.669238    22.665027    22.583023   \n",
       "1059    40.023809    35.621648   -62.879543    -4.518492    24.913797   \n",
       "2434    35.188715    32.291406   -69.272988    -0.820188    24.852734   \n",
       "15      17.370262    -0.471125    32.270395     9.672199    24.974848   \n",
       "1204    14.963180    20.307562    18.939926    13.221715    28.886793   \n",
       "\n",
       "      lag1_mean_d_h2h1_0  lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  \\\n",
       "2412          -18.878153            0.489474           16.780905   \n",
       "792            19.649232         -170.145955          720.924986   \n",
       "564            14.767124            0.064933           37.776384   \n",
       "1728           -8.435205            2.915673           -3.596188   \n",
       "1034          -16.266509            2.341476            8.174973   \n",
       "...                  ...                 ...                 ...   \n",
       "1935           59.786704           12.293798          -20.877694   \n",
       "1059            3.340981            0.509492           -8.996983   \n",
       "2434           -8.261413           -0.207049            9.248070   \n",
       "15              2.655548           33.326193          -17.298875   \n",
       "1204           -8.174696           -1.366989            0.052292   \n",
       "\n",
       "      lag1_mean_d_h2h1_3  lag1_mean_d_h2h1_4  ...  freq_659_4  freq_669_4  \\\n",
       "2412           -2.257252           -4.993806  ...    0.005472    0.013660   \n",
       "792            28.940793           -2.088506  ...    0.004709    0.002905   \n",
       "564             4.238803           -0.476757  ...    0.003072    0.010312   \n",
       "1728          -13.659502           -2.054757  ...    0.067564    0.026157   \n",
       "1034           -6.269097            4.000325  ...    0.040791    0.032998   \n",
       "...                  ...                 ...  ...         ...         ...   \n",
       "1935           59.814028           -3.487232  ...    0.009657    0.003790   \n",
       "1059            4.844432           -1.288017  ...    0.016422    0.047934   \n",
       "2434           -1.986913            5.773653  ...    0.022761    0.027900   \n",
       "15            -22.121268           -3.426928  ...    0.034116    0.019738   \n",
       "1204          -12.047434            6.061144  ...    0.045316    0.057409   \n",
       "\n",
       "      freq_679_4  freq_689_4  freq_699_4  freq_709_4  freq_720_4  freq_730_4  \\\n",
       "2412    0.031179    0.024357    0.028514    0.034570    0.014269    0.045188   \n",
       "792     0.002722    0.002205    0.006307    0.002322    0.005463    0.001628   \n",
       "564     0.012050    0.022987    0.014199    0.005947    0.012387    0.010299   \n",
       "1728    0.049743    0.067106    0.037596    0.029957    0.052189    0.070352   \n",
       "1034    0.035613    0.027868    0.089029    0.066754    0.087388    0.020152   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1935    0.016537    0.012653    0.017528    0.011030    0.007071    0.003956   \n",
       "1059    0.024865    0.034611    0.018285    0.013902    0.006733    0.020504   \n",
       "2434    0.023723    0.036557    0.022434    0.008206    0.001881    0.024254   \n",
       "15      0.013782    0.023540    0.014592    0.012901    0.023984    0.027613   \n",
       "1204    0.035277    0.022084    0.031079    0.090226    0.041514    0.030212   \n",
       "\n",
       "      freq_740_4  freq_750_4  \n",
       "2412    0.016358    0.022625  \n",
       "792     0.002900    0.004544  \n",
       "564     0.002835    0.000672  \n",
       "1728    0.057723    0.006037  \n",
       "1034    0.050728    0.065582  \n",
       "...          ...         ...  \n",
       "1935    0.009225    0.007511  \n",
       "1059    0.032040    0.008336  \n",
       "2434    0.038976    0.009429  \n",
       "15      0.017700    0.012127  \n",
       "1204    0.011472    0.051987  \n",
       "\n",
       "[744 rows x 1245 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2412    2.0\n",
       "792     1.0\n",
       "564     2.0\n",
       "1728    1.0\n",
       "1034    1.0\n",
       "       ... \n",
       "1935    2.0\n",
       "1059    2.0\n",
       "2434    2.0\n",
       "15      2.0\n",
       "1204    1.0\n",
       "Name: Label, Length: 744, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.3899962e-13, 5.4883975e-12, 1.0000000e+00],\n",
       "       [3.4665548e-13, 1.5631734e-08, 1.0000000e+00],\n",
       "       [3.3761520e-21, 1.1045016e-18, 1.0000000e+00],\n",
       "       ...,\n",
       "       [1.7209749e-18, 1.1452423e-15, 1.0000000e+00],\n",
       "       [2.6929051e-02, 1.6508151e-06, 9.7306937e-01],\n",
       "       [3.3333433e-11, 1.0000000e+00, 3.3385221e-15]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      RELAXED       0.92      0.94      0.93       252\n",
      "      NEUTRAL       0.92      0.87      0.89       246\n",
      "CONCENTRATING       0.95      0.97      0.96       246\n",
      "\n",
      "     accuracy                           0.93       744\n",
      "    macro avg       0.93      0.93      0.93       744\n",
      " weighted avg       0.93      0.93      0.93       744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = np.array(list(map(lambda x: np.argmax(x), model.predict(X_test))))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clr = classification_report(y_test, y_pred, target_names=label_mapping.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.yticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('1246_gru.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating over my collected data (7/10)\n",
    "1. Reformat the timestamp (÷ 1,000,000) that period of 1.0 can actually represent 1 second in the timestamp of the dataset (currently 1,000,000 represents one second period)\n",
    "2. Export the new dataset into my_new_data\n",
    "3. Perform feature extraction over my_new_data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_timestamp_n_export(raw_dir, new_dir):\n",
    "    \n",
    "    # ~/new_dataset\n",
    "    if not os.getcwd() + '/new_dataset':\n",
    "        os.mkdir('/new_dataset')\n",
    "        \n",
    "    # ~/new_dataset/filename        \n",
    "    if not os.path.exists(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    \n",
    "    files = os.listdir(raw_dir)\n",
    "    for file in files:\n",
    "        if '.csv' in file:\n",
    "            print(file)\n",
    "            df = pd.read_csv(raw_dir + '/' + file)\n",
    "            for i in range(df['timestamps'].shape[0]):\n",
    "                timestamp = df['timestamps'][i]/1000000.0\n",
    "                df['timestamps'][i] = timestamp\n",
    "            df.to_csv(new_dir + '/' + file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andyko/Desktop/EEG_Classification/raw_dataset/mental-state\n",
      "/Users/andyko/Desktop/EEG_Classification/new_dataset/mental-state\n",
      "subjecta-concentrating-1.csv\n",
      "subjecta-relaxed-1.csv\n",
      "subjecta-neutral-1.csv\n",
      "subjectb-neutral-1.csv\n"
     ]
    }
   ],
   "source": [
    "def new_dataset(filename):\n",
    "    \n",
    "    raw_dir = os.getcwd() + '/raw_dataset/' + filename\n",
    "    new_dir = os.getcwd() + '/new_dataset/' + os.path.basename(raw_dir)\n",
    "\n",
    "    print(raw_dir)\n",
    "    print(new_dir)\n",
    "    \n",
    "    reformat_timestamp_n_export(raw_dir, new_dir)\n",
    "    \n",
    "\n",
    "new_dataset('mental-state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_dir: C:\\Users\\14056\\Desktop\\EEG_Classification/new_dataset/mental-state\n",
      "subjecta-concentrating-1\n",
      "subjecta\n",
      "concentrating\n",
      "Using file subjecta-concentrating-1.csv\n",
      "resulting vector shape for the file (233, 1246)\n",
      "subjecta-neutral-1\n",
      "subjecta\n",
      "neutral\n",
      "Using file subjecta-neutral-1.csv\n",
      "resulting vector shape for the file (84, 1246)\n",
      "subjecta-relaxed-1\n",
      "subjecta\n",
      "relaxed\n",
      "Using file subjecta-relaxed-1.csv\n",
      "resulting vector shape for the file (72, 1246)\n",
      "subjectb-neutral-1\n",
      "subjectb\n",
      "neutral\n",
      "Using file subjectb-neutral-1.csv\n",
      "resulting vector shape for the file (77, 1246)\n",
      "FINAL_MATRIX (466, 1246)\n"
     ]
    }
   ],
   "source": [
    "import EEG_feature_extraction as efe\n",
    "from EEG_generate_training_matrix import gen_training_matrix\n",
    "\n",
    "filename = 'mental-state'\n",
    "raw_dir = os.getcwd() + '/raw_dataset/' + filename\n",
    "new_dir = os.getcwd() + '/new_dataset/' + os.path.basename(raw_dir)\n",
    "\n",
    "print('new_dir: {}'.format(new_dir))\n",
    "\n",
    "outfile_path = os.getcwd() + '/final_dataset/mental-state-test.csv'\n",
    "\n",
    "gen_training_matrix(new_dir, outfile_path, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466, 1246)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_d_h2h1_4</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_4</th>\n",
       "      <th>freq_679_4</th>\n",
       "      <th>freq_689_4</th>\n",
       "      <th>freq_699_4</th>\n",
       "      <th>freq_709_4</th>\n",
       "      <th>freq_720_4</th>\n",
       "      <th>freq_730_4</th>\n",
       "      <th>freq_740_4</th>\n",
       "      <th>freq_750_4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-32.835012</td>\n",
       "      <td>-29.756555</td>\n",
       "      <td>-30.616777</td>\n",
       "      <td>-22.864500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.226164</td>\n",
       "      <td>3.272177</td>\n",
       "      <td>1.113522</td>\n",
       "      <td>5.079928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-34.004223</td>\n",
       "      <td>-31.633387</td>\n",
       "      <td>-30.490902</td>\n",
       "      <td>-27.353816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.264217</td>\n",
       "      <td>-1.762242</td>\n",
       "      <td>1.005284</td>\n",
       "      <td>-2.336440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-31.890855</td>\n",
       "      <td>-29.411324</td>\n",
       "      <td>-27.668023</td>\n",
       "      <td>-22.442930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.517530</td>\n",
       "      <td>1.160086</td>\n",
       "      <td>-1.470060</td>\n",
       "      <td>-1.838488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.428933e-19</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>4.385062e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.365796e-19</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>4.351439e-19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-26.618988</td>\n",
       "      <td>-21.490086</td>\n",
       "      <td>-11.682508</td>\n",
       "      <td>-25.004465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.489235</td>\n",
       "      <td>3.382499</td>\n",
       "      <td>0.909437</td>\n",
       "      <td>13.872704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.749779e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-30.618672</td>\n",
       "      <td>-27.811039</td>\n",
       "      <td>-28.177250</td>\n",
       "      <td>-25.095496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.495357</td>\n",
       "      <td>-5.197282</td>\n",
       "      <td>0.154096</td>\n",
       "      <td>-7.354144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.857867e-19</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>8.770125e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.731593e-19</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>8.702878e-19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>-39.823535</td>\n",
       "      <td>-22.712707</td>\n",
       "      <td>-22.434262</td>\n",
       "      <td>-22.781309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.746433</td>\n",
       "      <td>0.571058</td>\n",
       "      <td>-1.886656</td>\n",
       "      <td>19.621902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.999118e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>-27.456258</td>\n",
       "      <td>-32.295211</td>\n",
       "      <td>-30.439395</td>\n",
       "      <td>-17.922121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230727</td>\n",
       "      <td>0.664401</td>\n",
       "      <td>0.077608</td>\n",
       "      <td>-2.692824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>-31.406418</td>\n",
       "      <td>-28.295527</td>\n",
       "      <td>-29.029816</td>\n",
       "      <td>-11.582398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.047367</td>\n",
       "      <td>-0.650401</td>\n",
       "      <td>-3.921702</td>\n",
       "      <td>1.118216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>-33.563598</td>\n",
       "      <td>-19.554113</td>\n",
       "      <td>-25.207520</td>\n",
       "      <td>-9.634711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.234329</td>\n",
       "      <td>7.990476</td>\n",
       "      <td>1.180159</td>\n",
       "      <td>4.693278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>-33.082965</td>\n",
       "      <td>-33.159246</td>\n",
       "      <td>-30.092215</td>\n",
       "      <td>-27.362250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218565</td>\n",
       "      <td>-1.799948</td>\n",
       "      <td>0.587557</td>\n",
       "      <td>-4.760864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 1246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "0     -32.835012   -29.756555   -30.616777   -22.864500          1.0   \n",
       "1     -34.004223   -31.633387   -30.490902   -27.353816          1.0   \n",
       "2     -31.890855   -29.411324   -27.668023   -22.442930          1.0   \n",
       "3     -26.618988   -21.490086   -11.682508   -25.004465          1.0   \n",
       "4     -30.618672   -27.811039   -28.177250   -25.095496          1.0   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "461   -39.823535   -22.712707   -22.434262   -22.781309          1.0   \n",
       "462   -27.456258   -32.295211   -30.439395   -17.922121          1.0   \n",
       "463   -31.406418   -28.295527   -29.029816   -11.582398          1.0   \n",
       "464   -33.563598   -19.554113   -25.207520    -9.634711          1.0   \n",
       "465   -33.082965   -33.159246   -30.092215   -27.362250          1.0   \n",
       "\n",
       "     lag1_mean_d_h2h1_0  lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  \\\n",
       "0              4.226164            3.272177            1.113522   \n",
       "1             -1.264217           -1.762242            1.005284   \n",
       "2             -2.517530            1.160086           -1.470060   \n",
       "3            -12.489235            3.382499            0.909437   \n",
       "4             -8.495357           -5.197282            0.154096   \n",
       "..                  ...                 ...                 ...   \n",
       "461           -5.746433            0.571058           -1.886656   \n",
       "462            0.230727            0.664401            0.077608   \n",
       "463            1.047367           -0.650401           -3.921702   \n",
       "464            1.234329            7.990476            1.180159   \n",
       "465            0.218565           -1.799948            0.587557   \n",
       "\n",
       "     lag1_mean_d_h2h1_3  lag1_mean_d_h2h1_4  ...    freq_669_4    freq_679_4  \\\n",
       "0              5.079928                 0.0  ...  0.000000e+00  3.569605e-19   \n",
       "1             -2.336440                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "2             -1.838488                 0.0  ...  4.428933e-19  3.569605e-19   \n",
       "3             13.872704                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "4             -7.354144                 0.0  ...  8.857867e-19  7.139210e-19   \n",
       "..                  ...                 ...  ...           ...           ...   \n",
       "461           19.621902                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "462           -2.692824                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "463            1.118216                 0.0  ...  0.000000e+00  7.139210e-19   \n",
       "464            4.693278                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "465           -4.760864                 0.0  ...  0.000000e+00  3.569605e-19   \n",
       "\n",
       "       freq_689_4    freq_699_4    freq_709_4    freq_720_4    freq_730_4  \\\n",
       "0    3.557783e-19  0.000000e+00  0.000000e+00  0.000000e+00  3.526575e-19   \n",
       "1    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "2    3.557783e-19  4.385062e-19  0.000000e+00  4.365796e-19  3.526575e-19   \n",
       "3    0.000000e+00  0.000000e+00  1.749779e-18  0.000000e+00  0.000000e+00   \n",
       "4    7.115566e-19  8.770125e-19  0.000000e+00  8.731593e-19  7.053150e-19   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "461  0.000000e+00  0.000000e+00  6.999118e-18  0.000000e+00  0.000000e+00   \n",
       "462  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "463  7.115566e-19  0.000000e+00  0.000000e+00  0.000000e+00  7.053150e-19   \n",
       "464  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "465  3.557783e-19  0.000000e+00  0.000000e+00  0.000000e+00  3.526575e-19   \n",
       "\n",
       "       freq_740_4    freq_750_4  Label  \n",
       "0    3.522706e-19  0.000000e+00    0.0  \n",
       "1    0.000000e+00  0.000000e+00    1.0  \n",
       "2    3.522706e-19  4.351439e-19    2.0  \n",
       "3    0.000000e+00  0.000000e+00    2.0  \n",
       "4    7.045412e-19  8.702878e-19    2.0  \n",
       "..            ...           ...    ...  \n",
       "461  0.000000e+00  0.000000e+00    2.0  \n",
       "462  0.000000e+00  0.000000e+00    1.0  \n",
       "463  7.045412e-19  0.000000e+00    2.0  \n",
       "464  0.000000e+00  0.000000e+00    2.0  \n",
       "465  3.522706e-19  0.000000e+00    0.0  \n",
       "\n",
       "[466 rows x 1246 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile_path = os.getcwd() + '/final_dataset/mental-state-test.csv'\n",
    "mental_state_test = pd.read_csv(outfile_path)\n",
    "print(mental_state_test.shape)\n",
    "mental_state_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1245)]            0         \n",
      "_________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)  (None, 1245, 1)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 1245, 256)         198912    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 318720)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 956163    \n",
      "=================================================================\n",
      "Total params: 1,155,075\n",
      "Trainable params: 1,155,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.getcwd() + '/saved_models/1246_gru.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    233\n",
       "1.0    161\n",
       "0.0     72\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mental_state_test['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    y = df['Label'].copy()\n",
    "    X = df.drop('Label', axis=1).copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(mental_state_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 35s 4s/step - loss: 12.2324 - accuracy: 0.4037 - val_loss: 2.9126 - val_accuracy: 0.6515\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 44s 5s/step - loss: 1.8849 - accuracy: 0.7791 - val_loss: 0.2732 - val_accuracy: 0.9697\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 46s 5s/step - loss: 0.6613 - accuracy: 0.8825 - val_loss: 0.1526 - val_accuracy: 0.9848\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 38s 4s/step - loss: 0.5345 - accuracy: 0.9097 - val_loss: 0.1679 - val_accuracy: 0.9697\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 37s 4s/step - loss: 0.1323 - accuracy: 0.9609 - val_loss: 0.2888 - val_accuracy: 0.9242\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 54s 6s/step - loss: 0.0715 - accuracy: 0.9768 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 45s 5s/step - loss: 0.0297 - accuracy: 0.9944 - val_loss: 0.0272 - val_accuracy: 0.9848\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 6.2520e-04 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9242\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 50s 6s/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.0966 - val_accuracy: 0.9848\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 39s 4s/step - loss: 2.5452e-04 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.429%\n"
     ]
    }
   ],
   "source": [
    "model_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Test Accuracy: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'RELAXED': 0, 'NEUTRAL': 1, 'CONCENTRATING': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_d_h2h1_4</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_659_4</th>\n",
       "      <th>freq_669_4</th>\n",
       "      <th>freq_679_4</th>\n",
       "      <th>freq_689_4</th>\n",
       "      <th>freq_699_4</th>\n",
       "      <th>freq_709_4</th>\n",
       "      <th>freq_720_4</th>\n",
       "      <th>freq_730_4</th>\n",
       "      <th>freq_740_4</th>\n",
       "      <th>freq_750_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-32.455426</td>\n",
       "      <td>-27.387637</td>\n",
       "      <td>-26.483512</td>\n",
       "      <td>-14.304102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231341</td>\n",
       "      <td>-1.737929</td>\n",
       "      <td>-2.387364</td>\n",
       "      <td>-0.452512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>8.857867e-19</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>8.770125e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.731593e-19</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>8.702878e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-30.672043</td>\n",
       "      <td>-33.586500</td>\n",
       "      <td>-31.854602</td>\n",
       "      <td>-17.557855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.136108</td>\n",
       "      <td>2.939708</td>\n",
       "      <td>2.169119</td>\n",
       "      <td>-1.259762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-32.117820</td>\n",
       "      <td>-32.724395</td>\n",
       "      <td>-23.533906</td>\n",
       "      <td>-15.120004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.968627</td>\n",
       "      <td>2.252739</td>\n",
       "      <td>-3.183865</td>\n",
       "      <td>-1.425589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-31.429301</td>\n",
       "      <td>-29.716477</td>\n",
       "      <td>-26.407211</td>\n",
       "      <td>-27.679992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.029010</td>\n",
       "      <td>-1.367928</td>\n",
       "      <td>-0.267784</td>\n",
       "      <td>-3.957018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.857867e-19</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>8.770125e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.731593e-19</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>8.702878e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-30.830402</td>\n",
       "      <td>-26.315711</td>\n",
       "      <td>-29.893883</td>\n",
       "      <td>-26.161711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>-0.987826</td>\n",
       "      <td>4.649445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-30.963910</td>\n",
       "      <td>-27.204500</td>\n",
       "      <td>-29.302582</td>\n",
       "      <td>-27.185953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.359696</td>\n",
       "      <td>-0.949526</td>\n",
       "      <td>0.097314</td>\n",
       "      <td>-2.809656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>-32.785410</td>\n",
       "      <td>-29.829027</td>\n",
       "      <td>-30.471773</td>\n",
       "      <td>-18.140461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.734281</td>\n",
       "      <td>-1.987828</td>\n",
       "      <td>-1.110879</td>\n",
       "      <td>2.849805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>8.857867e-19</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>8.770125e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.731593e-19</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>8.702878e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-26.647598</td>\n",
       "      <td>-29.954926</td>\n",
       "      <td>-30.775055</td>\n",
       "      <td>-22.487027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.947521</td>\n",
       "      <td>-4.308325</td>\n",
       "      <td>-2.473104</td>\n",
       "      <td>4.424934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>8.857867e-19</td>\n",
       "      <td>7.139210e-19</td>\n",
       "      <td>7.115566e-19</td>\n",
       "      <td>8.770125e-19</td>\n",
       "      <td>3.499559e-18</td>\n",
       "      <td>8.731593e-19</td>\n",
       "      <td>7.053150e-19</td>\n",
       "      <td>7.045412e-19</td>\n",
       "      <td>8.702878e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-31.187062</td>\n",
       "      <td>-27.576438</td>\n",
       "      <td>-29.302633</td>\n",
       "      <td>-26.566094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.427399</td>\n",
       "      <td>1.390979</td>\n",
       "      <td>2.577131</td>\n",
       "      <td>7.533329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-30.050277</td>\n",
       "      <td>-29.031754</td>\n",
       "      <td>-29.106156</td>\n",
       "      <td>-25.035297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.806412</td>\n",
       "      <td>-0.555078</td>\n",
       "      <td>0.435603</td>\n",
       "      <td>3.296843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.878597e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.569605e-19</td>\n",
       "      <td>3.557783e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.499559e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.526575e-19</td>\n",
       "      <td>3.522706e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 1245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "344   -32.455426   -27.387637   -26.483512   -14.304102          1.0   \n",
       "33    -30.672043   -33.586500   -31.854602   -17.557855          1.0   \n",
       "162   -32.117820   -32.724395   -23.533906   -15.120004          1.0   \n",
       "11    -31.429301   -29.716477   -26.407211   -27.679992          1.0   \n",
       "223   -30.830402   -26.315711   -29.893883   -26.161711          1.0   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "309   -30.963910   -27.204500   -29.302582   -27.185953          1.0   \n",
       "377   -32.785410   -29.829027   -30.471773   -18.140461          1.0   \n",
       "196   -26.647598   -29.954926   -30.775055   -22.487027          1.0   \n",
       "171   -31.187062   -27.576438   -29.302633   -26.566094          1.0   \n",
       "185   -30.050277   -29.031754   -29.106156   -25.035297          1.0   \n",
       "\n",
       "     lag1_mean_d_h2h1_0  lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  \\\n",
       "344            0.231341           -1.737929           -2.387364   \n",
       "33             3.136108            2.939708            2.169119   \n",
       "162           -2.968627            2.252739           -3.183865   \n",
       "11             1.029010           -1.367928           -0.267784   \n",
       "223           -0.039551           -0.453968           -0.987826   \n",
       "..                  ...                 ...                 ...   \n",
       "309           -0.359696           -0.949526            0.097314   \n",
       "377           -2.734281           -1.987828           -1.110879   \n",
       "196           -1.947521           -4.308325           -2.473104   \n",
       "171            8.427399            1.390979            2.577131   \n",
       "185            1.806412           -0.555078            0.435603   \n",
       "\n",
       "     lag1_mean_d_h2h1_3  lag1_mean_d_h2h1_4  ...    freq_659_4    freq_669_4  \\\n",
       "344           -0.452512                 0.0  ...  2.878597e-18  8.857867e-19   \n",
       "33            -1.259762                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "162           -1.425589                 0.0  ...  2.878597e-18  0.000000e+00   \n",
       "11            -3.957018                 0.0  ...  0.000000e+00  8.857867e-19   \n",
       "223            4.649445                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "..                  ...                 ...  ...           ...           ...   \n",
       "309           -2.809656                 0.0  ...  2.878597e-18  0.000000e+00   \n",
       "377            2.849805                 0.0  ...  2.878597e-18  8.857867e-19   \n",
       "196            4.424934                 0.0  ...  2.878597e-18  8.857867e-19   \n",
       "171            7.533329                 0.0  ...  0.000000e+00  0.000000e+00   \n",
       "185            3.296843                 0.0  ...  2.878597e-18  0.000000e+00   \n",
       "\n",
       "       freq_679_4    freq_689_4    freq_699_4    freq_709_4    freq_720_4  \\\n",
       "344  7.139210e-19  7.115566e-19  8.770125e-19  0.000000e+00  8.731593e-19   \n",
       "33   7.139210e-19  7.115566e-19  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "162  7.139210e-19  7.115566e-19  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "11   7.139210e-19  7.115566e-19  8.770125e-19  0.000000e+00  8.731593e-19   \n",
       "223  3.569605e-19  3.557783e-19  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "309  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "377  7.139210e-19  7.115566e-19  8.770125e-19  0.000000e+00  8.731593e-19   \n",
       "196  7.139210e-19  7.115566e-19  8.770125e-19  3.499559e-18  8.731593e-19   \n",
       "171  3.569605e-19  3.557783e-19  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "185  3.569605e-19  3.557783e-19  0.000000e+00  3.499559e-18  0.000000e+00   \n",
       "\n",
       "       freq_730_4    freq_740_4    freq_750_4  \n",
       "344  7.053150e-19  7.045412e-19  8.702878e-19  \n",
       "33   7.053150e-19  7.045412e-19  0.000000e+00  \n",
       "162  7.053150e-19  7.045412e-19  0.000000e+00  \n",
       "11   7.053150e-19  7.045412e-19  8.702878e-19  \n",
       "223  3.526575e-19  3.522706e-19  0.000000e+00  \n",
       "..            ...           ...           ...  \n",
       "309  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "377  7.053150e-19  7.045412e-19  8.702878e-19  \n",
       "196  7.053150e-19  7.045412e-19  8.702878e-19  \n",
       "171  3.526575e-19  3.522706e-19  0.000000e+00  \n",
       "185  3.526575e-19  3.522706e-19  0.000000e+00  \n",
       "\n",
       "[140 rows x 1245 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344    2.0\n",
       "33     1.0\n",
       "162    2.0\n",
       "11     2.0\n",
       "223    0.0\n",
       "      ... \n",
       "309    0.0\n",
       "377    1.0\n",
       "196    1.0\n",
       "171    0.0\n",
       "185    0.0\n",
       "Name: Label, Length: 140, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.67897267e-08, 2.06228094e-14, 1.00000000e+00],\n",
       "       [2.19998575e-10, 1.00000000e+00, 6.62534264e-14],\n",
       "       [1.13412483e-07, 1.23027551e-13, 9.99999881e-01],\n",
       "       [2.01320168e-22, 1.82161690e-21, 1.00000000e+00],\n",
       "       [9.62310731e-01, 2.99265508e-11, 3.76892127e-02],\n",
       "       [9.99996066e-01, 2.49721406e-16, 3.90659352e-06],\n",
       "       [2.67723144e-15, 8.44221567e-20, 1.00000000e+00],\n",
       "       [3.18668380e-16, 5.35057670e-22, 1.00000000e+00],\n",
       "       [1.13468089e-28, 3.57666328e-28, 1.00000000e+00],\n",
       "       [6.03377259e-08, 3.22137672e-12, 9.99999881e-01],\n",
       "       [3.88591381e-09, 1.00000000e+00, 2.12292778e-15],\n",
       "       [8.94335628e-01, 7.51129846e-07, 1.05663635e-01],\n",
       "       [8.13212812e-01, 1.56192311e-06, 1.86785638e-01],\n",
       "       [2.33522202e-10, 2.57357437e-18, 1.00000000e+00],\n",
       "       [2.55397588e-25, 5.59761057e-22, 1.00000000e+00],\n",
       "       [3.57108102e-24, 1.16540007e-23, 1.00000000e+00],\n",
       "       [5.56896111e-07, 2.20683784e-11, 9.99999404e-01],\n",
       "       [3.01040153e-08, 4.79567125e-17, 1.00000000e+00],\n",
       "       [1.79978382e-28, 8.17952022e-33, 1.00000000e+00],\n",
       "       [1.81416362e-12, 5.03195639e-18, 1.00000000e+00],\n",
       "       [5.10928268e-20, 1.00000000e+00, 7.08877401e-13],\n",
       "       [3.01230360e-11, 2.99141078e-15, 1.00000000e+00],\n",
       "       [7.51980930e-04, 9.99247789e-01, 1.84326268e-07],\n",
       "       [4.09893719e-09, 4.93235756e-16, 1.00000000e+00],\n",
       "       [5.56284352e-09, 1.45982488e-12, 1.00000000e+00],\n",
       "       [3.88839760e-09, 1.00000000e+00, 5.29346011e-10],\n",
       "       [1.65091683e-06, 5.90102616e-07, 9.99997735e-01],\n",
       "       [1.92104571e-16, 1.55756769e-10, 1.00000000e+00],\n",
       "       [2.08198888e-12, 1.00000000e+00, 6.73677173e-16],\n",
       "       [3.14330201e-10, 1.00000000e+00, 1.05940104e-10],\n",
       "       [1.36308684e-18, 8.66324735e-27, 1.00000000e+00],\n",
       "       [6.98718083e-10, 1.00000000e+00, 9.90303325e-11],\n",
       "       [1.92331637e-11, 1.79545820e-19, 1.00000000e+00],\n",
       "       [5.08363100e-12, 1.00000000e+00, 2.67551641e-11],\n",
       "       [3.70006859e-07, 3.75312947e-14, 9.99999642e-01],\n",
       "       [8.69356275e-01, 1.35719990e-17, 1.30643740e-01],\n",
       "       [2.55716961e-28, 2.53245729e-18, 1.00000000e+00],\n",
       "       [0.00000000e+00, 1.20768239e-27, 1.00000000e+00],\n",
       "       [9.02435102e-04, 2.45919396e-10, 9.99097586e-01],\n",
       "       [9.38275607e-13, 8.91601584e-11, 1.00000000e+00],\n",
       "       [3.68169606e-01, 2.92516076e-12, 6.31830394e-01],\n",
       "       [9.98194754e-01, 1.16068111e-06, 1.80410862e-03],\n",
       "       [1.70602047e-17, 7.82517044e-24, 1.00000000e+00],\n",
       "       [6.01888195e-14, 9.99654889e-01, 3.45079607e-04],\n",
       "       [5.42327962e-06, 5.65870094e-12, 9.99994636e-01],\n",
       "       [2.38929351e-04, 7.77723748e-17, 9.99761045e-01],\n",
       "       [3.92343991e-10, 1.00000000e+00, 1.75795958e-11],\n",
       "       [9.19154672e-08, 9.99999881e-01, 4.58217931e-15],\n",
       "       [2.82609832e-14, 2.78896723e-18, 1.00000000e+00],\n",
       "       [9.54521966e-19, 1.81932900e-17, 1.00000000e+00],\n",
       "       [8.73115084e-07, 9.99999166e-01, 1.25903921e-09],\n",
       "       [1.22232558e-24, 1.00000000e+00, 3.82712850e-20],\n",
       "       [1.42456749e-08, 1.00000000e+00, 3.24330059e-12],\n",
       "       [2.53476144e-04, 9.99746501e-01, 6.07083550e-10],\n",
       "       [3.12206367e-08, 6.63227046e-18, 1.00000000e+00],\n",
       "       [8.84787312e-07, 9.99999166e-01, 6.39372019e-13],\n",
       "       [1.73979679e-05, 1.09508192e-12, 9.99982595e-01],\n",
       "       [2.56950494e-13, 1.47820807e-12, 1.00000000e+00],\n",
       "       [7.66000197e-12, 1.75762690e-23, 1.00000000e+00],\n",
       "       [9.72884134e-11, 9.99980569e-01, 1.93967480e-05],\n",
       "       [8.48626427e-04, 9.69770730e-01, 2.93805730e-02],\n",
       "       [5.41705847e-10, 1.17393218e-14, 1.00000000e+00],\n",
       "       [4.70706851e-08, 2.89915919e-01, 7.10084081e-01],\n",
       "       [1.76194760e-07, 5.26487048e-11, 9.99999881e-01],\n",
       "       [1.12523955e-08, 2.72267158e-15, 1.00000000e+00],\n",
       "       [5.09996861e-02, 9.49000359e-01, 3.91734605e-12],\n",
       "       [9.65474034e-10, 1.00000000e+00, 8.98175035e-14],\n",
       "       [6.82483315e-01, 1.94209818e-07, 3.17516536e-01],\n",
       "       [1.12712222e-10, 1.82252062e-21, 1.00000000e+00],\n",
       "       [7.01214015e-01, 2.98786014e-01, 2.27276267e-10],\n",
       "       [2.50394816e-09, 2.25754682e-09, 1.00000000e+00],\n",
       "       [3.65990281e-01, 6.34009719e-01, 1.40816503e-13],\n",
       "       [1.00000000e+00, 7.83871162e-11, 3.10021431e-12],\n",
       "       [9.99999642e-01, 9.10183997e-15, 3.02527553e-07],\n",
       "       [9.40312272e-10, 1.00000000e+00, 1.07481949e-12],\n",
       "       [2.32568977e-12, 1.00000000e+00, 2.16961633e-17],\n",
       "       [3.59298022e-34, 0.00000000e+00, 1.00000000e+00],\n",
       "       [6.00382544e-09, 3.50835264e-01, 6.49164736e-01],\n",
       "       [9.99995470e-01, 2.17008495e-11, 4.58081649e-06],\n",
       "       [9.40491850e-17, 1.00000000e+00, 1.00472191e-12],\n",
       "       [1.80681826e-15, 5.78302752e-05, 9.99942183e-01],\n",
       "       [4.21616346e-01, 3.50676419e-05, 5.78348637e-01],\n",
       "       [2.06386297e-10, 2.29323028e-12, 1.00000000e+00],\n",
       "       [3.17797221e-13, 1.00000000e+00, 2.35761314e-17],\n",
       "       [1.28910849e-02, 2.47304645e-13, 9.87108946e-01],\n",
       "       [2.93475099e-21, 9.12530709e-08, 9.99999881e-01],\n",
       "       [2.39385373e-10, 1.00000000e+00, 7.08751981e-12],\n",
       "       [2.12431386e-01, 7.87532449e-01, 3.62434366e-05],\n",
       "       [1.26038622e-12, 1.00000000e+00, 1.05309952e-17],\n",
       "       [3.83058900e-13, 1.00000000e+00, 3.30809152e-18],\n",
       "       [7.29250158e-23, 5.42768816e-22, 1.00000000e+00],\n",
       "       [6.45547865e-15, 1.00000000e+00, 3.88170658e-20],\n",
       "       [9.19369340e-01, 3.38923917e-10, 8.06307122e-02],\n",
       "       [4.83285650e-26, 5.42454080e-30, 1.00000000e+00],\n",
       "       [3.80058385e-09, 2.83830781e-17, 1.00000000e+00],\n",
       "       [4.65449262e-10, 1.00000000e+00, 9.84892398e-13],\n",
       "       [1.73448352e-04, 9.99826491e-01, 7.62183419e-08],\n",
       "       [4.17624542e-04, 2.64419070e-10, 9.99582350e-01],\n",
       "       [9.85173422e-22, 2.77106988e-23, 1.00000000e+00],\n",
       "       [9.99856830e-01, 3.69012654e-10, 1.43116908e-04],\n",
       "       [1.53364828e-02, 3.80319598e-08, 9.84663486e-01],\n",
       "       [2.18752811e-11, 1.58862276e-23, 1.00000000e+00],\n",
       "       [5.07558751e-09, 4.38216476e-15, 1.00000000e+00],\n",
       "       [1.99657507e-06, 1.98514745e-01, 8.01483333e-01],\n",
       "       [1.43756466e-07, 9.99999881e-01, 1.83491113e-12],\n",
       "       [5.85188924e-17, 2.39071492e-17, 1.00000000e+00],\n",
       "       [5.32060540e-06, 4.73560820e-21, 9.99994636e-01],\n",
       "       [2.46002135e-10, 1.00000000e+00, 1.44119390e-13],\n",
       "       [1.16632279e-12, 2.44438488e-18, 1.00000000e+00],\n",
       "       [9.36642647e-01, 3.48231985e-08, 6.33574054e-02],\n",
       "       [1.22827460e-11, 1.00000000e+00, 3.18632068e-15],\n",
       "       [4.83839959e-02, 1.14562589e-19, 9.51615989e-01],\n",
       "       [2.90944894e-13, 1.00000000e+00, 1.91982863e-17],\n",
       "       [0.00000000e+00, 1.25853732e-37, 1.00000000e+00],\n",
       "       [9.99964595e-01, 2.76014969e-14, 3.54502445e-05],\n",
       "       [4.01706527e-13, 4.39307645e-23, 1.00000000e+00],\n",
       "       [1.22551482e-05, 9.99987721e-01, 2.76178440e-13],\n",
       "       [2.35136729e-04, 2.38881396e-16, 9.99764860e-01],\n",
       "       [3.90994633e-11, 6.29749142e-14, 1.00000000e+00],\n",
       "       [2.35652828e-06, 8.19990944e-16, 9.99997616e-01],\n",
       "       [5.21606369e-11, 1.00000000e+00, 6.83856249e-10],\n",
       "       [5.33677312e-12, 4.99023075e-24, 1.00000000e+00],\n",
       "       [3.48998457e-01, 1.05714735e-08, 6.51001573e-01],\n",
       "       [5.14956078e-09, 4.47600195e-14, 1.00000000e+00],\n",
       "       [1.73022823e-20, 1.21923191e-19, 1.00000000e+00],\n",
       "       [6.31117242e-11, 2.42949771e-10, 1.00000000e+00],\n",
       "       [9.78658560e-13, 4.54933180e-15, 1.00000000e+00],\n",
       "       [2.54572296e-12, 5.03405026e-05, 9.99949694e-01],\n",
       "       [3.97229504e-13, 3.87213118e-16, 1.00000000e+00],\n",
       "       [9.99343097e-01, 3.26279818e-16, 6.56904187e-04],\n",
       "       [4.57861171e-09, 1.00000000e+00, 6.10216391e-11],\n",
       "       [2.89471373e-05, 2.93029490e-12, 9.99971032e-01],\n",
       "       [9.99994159e-01, 2.12003554e-14, 5.82395614e-06],\n",
       "       [3.15930396e-01, 6.84013486e-01, 5.60796798e-05],\n",
       "       [1.52681512e-29, 2.04253033e-19, 1.00000000e+00],\n",
       "       [6.73656702e-01, 1.28163530e-11, 3.26343358e-01],\n",
       "       [9.27976446e-12, 1.00000000e+00, 1.32640224e-16],\n",
       "       [9.96920688e-12, 1.00000000e+00, 7.39469666e-11],\n",
       "       [2.42381170e-03, 6.56080164e-18, 9.97576177e-01],\n",
       "       [9.99994278e-01, 8.79844895e-12, 5.69791155e-06]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      RELAXED       0.95      0.67      0.78        27\n",
      "      NEUTRAL       1.00      0.93      0.97        46\n",
      "CONCENTRATING       0.86      1.00      0.92        67\n",
      "\n",
      "     accuracy                           0.91       140\n",
      "    macro avg       0.94      0.87      0.89       140\n",
      " weighted avg       0.92      0.91      0.91       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = np.array(list(map(lambda x: np.argmax(x), model.predict(X_test))))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clr = classification_report(y_test, y_pred, target_names=label_mapping.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.yticks(np.arange(3) + 0.5, label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.getcwd() + '/saved_models/1246_gru_tested.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the state with my dataset\n",
    "7/11 - Not able to load the model on a different device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andyko/Desktop/EEG_Classification/saved_models/1246_gru_tested.h5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() + '/saved_models/1246_gru_tested.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8aa6b29cfd5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/saved_models/1246_gru_tested.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m       if (h5py is not None and\n\u001b[1;32m    205\u001b[0m           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 206\u001b[0;31m         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    207\u001b[0m                                                 compile)\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    184\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    171\u001b[0m   \"\"\"\n\u001b[1;32m    172\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    174\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return cls.from_config(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0mimproperly\u001b[0m \u001b[0mformatted\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \"\"\"\n\u001b[0;32m--> 668\u001b[0;31m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0m\u001b[1;32m    669\u001b[0m         config, custom_objects)\n\u001b[1;32m    670\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1283\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m           \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         input_tensors = (\n\u001b[1;32m   1232\u001b[0m             base_layer_utils.unnest_if_single_tensor(input_tensors))\n\u001b[0;32m-> 1233\u001b[0;31m       \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0;31m# Update node index map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# GRU does not support constants. Ignore it during process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[1;32m    643\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[1;32m    644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state_for_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2985\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2987\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   3003\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2998\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2819\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2820\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2866\u001b[0m           \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m           \u001b[0;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2804\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2805\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \"\"\"\n\u001b[0;32m-> 3030\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3031\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.getcwd() + '/saved_models/1246_gru_tested.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
