{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Issue 7/3\n",
    "1. Determine how many samples to use (segments)\n",
    "2. Determine the exact artifact removal method\n",
    "3. Increase the overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mne\n",
    "import random\n",
    "\n",
    "## Feature extraction method\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Using a person's 3 different mental states, relaxed, neutral, concetrating<br>\n",
    "- `trials = 3, channels = 4, samples = 15204 * 0.75, kernels = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo 1, using a subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_a_relaxed' shape: (15204, 6)\n",
      "sub_a_neutral' shape: (15204, 6)\n",
      "sub_a_concentrating' shape: (15192, 6)\n"
     ]
    }
   ],
   "source": [
    "sub_a_relaxed = pd.read_csv(os.getcwd() + '/eeg-feature-generation/dataset/original_data/subjecta-relaxed-1.csv')\n",
    "print('sub_a_relaxed\\' shape: {}'.format(sub_a_relaxed.shape))\n",
    "\n",
    "sub_a_neutral = pd.read_csv(os.getcwd() + '/eeg-feature-generation/dataset/original_data/subjecta-neutral-1.csv')\n",
    "print('sub_a_neutral\\' shape: {}'.format(sub_a_neutral.shape))\n",
    "\n",
    "sub_a_concentrating = pd.read_csv(os.getcwd() + '/eeg-feature-generation/dataset/original_data/subjecta-concentrating-1.csv')\n",
    "print('sub_a_concentrating\\' shape: {}'.format(sub_a_concentrating.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo 2, using the whole dataset\n",
    "- Matched the number of samples to 888, the df with the lowest number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjecta-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectd-neutral-2.csv, shape = ((888, 4))\n",
      "subjectd-neutral-1.csv, shape = ((888, 4))\n",
      "subjecta-neutral-1.csv, shape = ((888, 4))\n",
      "subjectb-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectb-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectb-neutral-2.csv, shape = ((888, 4))\n",
      "subjectb-neutral-1.csv, shape = ((888, 4))\n",
      "subjectd-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectd-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectc-neutral-1.csv, shape = ((888, 4))\n",
      "subjecta-neutral-2.csv, shape = ((888, 4))\n",
      "subjecta-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectc-concentrating-1.csv, shape = ((888, 4))\n",
      "subjectb-concentrating-1.csv, shape = ((888, 4))\n",
      "subjecta-concentrating-2.csv, shape = ((888, 4))\n",
      "subjectc-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectd-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectc-neutral-2.csv, shape = ((888, 4))\n",
      "subjecta-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectd-relaxed-1.csv, shape = ((888, 4))\n",
      "subjectc-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectb-relaxed-2.csv, shape = ((888, 4))\n",
      "subjectc-concentrating-2.csv, shape = ((888, 4))\n"
     ]
    }
   ],
   "source": [
    "directory = os.getcwd() + '/eeg-feature-generation/dataset/original_data/'\n",
    "files = os.listdir(directory)\n",
    "random.shuffle(files)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for filename in files:\n",
    "    if 'subject' in filename:\n",
    "        df = pd.read_csv(directory + filename)\n",
    "        df = df.drop(columns=['timestamps','Right AUX'])\n",
    "        df = df[:888]\n",
    "        if 'relaxed' in filename:\n",
    "            labels.append(0)\n",
    "        elif 'neutral' in filename:\n",
    "            labels.append(1)\n",
    "        elif 'concentrating' in filename:\n",
    "            labels.append(2)\n",
    "        data.append(df)\n",
    "        if len(df) < 15192:\n",
    "            print('{}, shape = ({})'.format(filename, df.shape))\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4, 888)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 59.105,  62.012,  44.922, ...,  15.625,  34.18 ,  45.898],\n",
       "        [ 28.32 ,  30.273,  30.273, ...,  31.738,  35.156,  35.156],\n",
       "        [ 15.137,  43.945, -97.656, ..., -60.059,  23.926, 112.793],\n",
       "        [ 12.207,  11.719,  11.23 , ...,   8.789,   8.789,  13.184]],\n",
       "\n",
       "       [[ -1.465,   8.301,  20.996, ...,  14.648,  12.207,  17.578],\n",
       "        [  6.836,   6.348,   7.812, ...,  13.672,  10.742,  10.254],\n",
       "        [ 12.695,  -2.93 ,   0.488, ...,  14.648,   5.859,  -3.906],\n",
       "        [-77.148, -81.055, -75.195, ..., -77.148, -78.613, -78.613]],\n",
       "\n",
       "       [[ 53.223,  45.41 ,  34.18 , ...,  25.879,  43.457,  40.527],\n",
       "        [ 23.926,  20.996,  19.043, ...,  30.762,  29.297,  26.855],\n",
       "        [ 14.16 ,  14.16 ,   5.371, ...,  24.414,  30.273,  25.879],\n",
       "        [  0.977,   5.371,   4.395, ..., -13.184,  -4.395,   0.977]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 18.066,  33.691,  24.902, ...,  -0.488,   0.977,  22.949],\n",
       "        [ 11.719,  20.996,  21.973, ...,  19.043,  13.672,  12.695],\n",
       "        [ -4.395,   2.441,   6.836, ...,  14.648,   4.883,   8.789],\n",
       "        [ -4.883,   1.953,   5.859, ...,   4.395,  -4.883, -10.254]],\n",
       "\n",
       "       [[ 20.996,  40.039,  48.34 , ...,  33.203,  47.852,  36.621],\n",
       "        [ 23.926,  22.949,  24.902, ...,  37.109,  35.156,  35.156],\n",
       "        [ 29.297,  28.809,  31.25 , ...,  24.902,  20.508,  18.066],\n",
       "        [ 20.02 ,  21.973,  28.32 , ...,  20.996,  26.367,  21.973]],\n",
       "\n",
       "       [[ 17.09 ,  32.227,  36.621, ...,  32.227,  32.715,  24.414],\n",
       "        [ 21.484, -12.695,  23.438, ..., 115.234,  72.266, -10.742],\n",
       "        [ 21.484,  16.602,  16.113, ...,  39.062,  29.297,  19.043],\n",
       "        [ 14.16 ,  17.09 ,  21.973, ...,  15.137,  17.09 ,  18.555]]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([np.transpose(df) for df in data])\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label\n",
    "- 0.0 = relaxed<br>\n",
    "- 1.0 = neutral<br>\n",
    "- 2.0 = concetrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- Convert data to NHWC (trials, channels, samples, kernels) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "- Drop unnecessary columns = `['timestamps','Right AUX']`\n",
    "- Match the number of samples = `15192`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_a_relaxed = sub_a_relaxed.drop(columns=['timestamps','Right AUX'])\n",
    "sub_a_neutral = sub_a_neutral.drop(columns=['timestamps','Right AUX'])\n",
    "sub_a_concentrating = sub_a_concentrating.drop(columns=['timestamps','Right AUX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_a_relaxed' shape: (15192, 4)\n",
      "sub_a_neutral' shape: (15192, 4)\n",
      "sub_a_concentrating' shape: (15192, 4)\n"
     ]
    }
   ],
   "source": [
    "sub_a_relaxed = sub_a_relaxed[:15192]\n",
    "sub_a_neutral = sub_a_neutral[:15192]\n",
    "\n",
    "print('sub_a_relaxed\\' shape: {}'.format(sub_a_relaxed.shape))\n",
    "print('sub_a_neutral\\' shape: {}'.format(sub_a_neutral.shape))\n",
    "print('sub_a_concentrating\\' shape: {}'.format(sub_a_concentrating.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 15192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 30.762,  26.367,  21.484, ...,  21.484,  20.996,  33.203],\n",
       "        [ 15.625,  13.672,  14.16 , ...,  22.949,  21.973,  22.461],\n",
       "        [ 29.785,  28.32 ,  28.32 , ...,  34.18 ,  33.691,  31.738],\n",
       "        [  0.977,   0.   ,   2.93 , ...,  11.23 ,   5.859,  10.254]],\n",
       "\n",
       "       [[  4.883,  19.531,  20.508, ...,  34.668,  35.156,  20.02 ],\n",
       "        [ 22.949,  22.461,  19.531, ...,  33.203,  32.715,  30.273],\n",
       "        [  8.789,   5.371,  11.23 , ...,  16.602,  18.555,  10.254],\n",
       "        [ 23.926,  28.32 ,  25.391, ...,  25.391,  35.156,  18.555]],\n",
       "\n",
       "       [[ 59.105,  62.012,  44.922, ...,  48.828,  50.293,  45.41 ],\n",
       "        [ 28.32 ,  30.273,  30.273, ...,  31.25 ,  31.25 ,  30.273],\n",
       "        [ 15.137,  43.945, -97.656, ..., 258.789, 264.16 ,  27.344],\n",
       "        [ 12.207,  11.719,  11.23 , ...,  38.086,  39.551,  39.062]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([np.transpose(sub_a_relaxed), np.transpose(sub_a_neutral), np.transpose(sub_a_concentrating)])\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label\n",
    "- 0.0 = relaxed<br>\n",
    "- 1.0 = neutral<br>\n",
    "- 2.0 = concetrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([0.0, 1.0, 2.0])\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction - Denoising\n",
    "Fast Fourier Transform (FFT) - A method to remove the artifact from the recorded data, such as eye blinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  6.57721652e-05  1.31544330e-04 ... -1.97316496e-04\n",
      " -1.31544330e-04 -6.57721652e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cc1784ed30>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy.fft as fft\n",
    "spectrum = fft.fft(sub_a_relaxed['TP9'], sub_a_relaxed.shape[0])\n",
    "freq = fft.fftfreq(len(spectrum))\n",
    "print(freq)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(freq, abs(spectrum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - EEGNet (Proven to be effective in classifying EEG Data)\n",
    "Parameter settings so far: nb_classes=3, as there are 3 classes to classify: relaxed, neutral, concentrating<br>\n",
    "\n",
    "Paper - https://arxiv.org/pdf/1611.08024.pdf <br>\n",
    "Github - https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Demo with 15192 samples (1 min period of mental state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 4, 15192, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 15192, 8)       60768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 15192, 8)       32        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 1, 15192, 16)      64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1, 15192, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1, 15192, 16)      0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 1, 3798, 16)       512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1, 3798, 16)       64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1, 3798, 16)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 1, 474, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 474, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7584)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 22755     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 84,259\n",
      "Trainable params: 84,179\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "model  = EEGNet(nb_classes=3, Chans=4, Samples=X.shape[2],\n",
    "                dropoutRate = 0.5, kernLength=int(X.shape[2]/2), \n",
    "                F1=8, D=2, F2=16, dropoutType = 'Dropout')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 15192, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 9s 9s/step - loss: 1.2772 - accuracy: 0.6667\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.7128 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.2945 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.1381 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0612 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc216efdc0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_b_relaxed shape: (15192, 4)\n"
     ]
    }
   ],
   "source": [
    "# Using as a test dataset\n",
    "sub_b_relaxed = pd.read_csv(os.getcwd() + '/eeg-feature-generation/dataset/original_data/subjectb-relaxed-1.csv')\n",
    "sub_b_relaxed = sub_b_relaxed.drop(columns=['timestamps','Right AUX'])\n",
    "sub_b_relaxed = sub_b_relaxed[:15192]\n",
    "print('sub_b_relaxed shape: {}'.format(sub_b_relaxed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 15192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 43.457,  57.617,  41.504, ...,  17.09 ,  21.973,  47.363],\n",
       "        [ 39.551,  40.039,  41.016, ...,  22.949,  30.762,  35.156],\n",
       "        [ 48.828,  48.34 ,  47.852, ...,  22.949,  20.508,  27.832],\n",
       "        [ 19.043,  23.438,  25.879, ..., -19.043, -20.508, -13.672]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([np.transpose(sub_b_relaxed)])\n",
    "print(X_test.shape)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 15192, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.array([0])\n",
    "print(Y_test.shape)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np_utils.to_categorical(Y_test, num_classes=3)\n",
    "print(Y_test.shape)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10493065, 0.13374467, 0.7613246 ]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(X_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = probs.argmax(axis = -1)  \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.000000 \n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('suba-1_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, chans, samples = 1, 4, 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 4, 888, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 888, 8)         3552      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 4, 888, 8)         32        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 1, 888, 16)        64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 1, 888, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1, 888, 16)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_12 (Averag (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 1, 222, 16)        512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1, 222, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1, 222, 16)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_13 (Averag (None, 1, 27, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 27, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 432)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1299      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 5,587\n",
      "Trainable params: 5,507\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
    "model  = EEGNet(nb_classes=3, Chans=chans, Samples=samples,\n",
    "                dropoutRate = 0.5, kernLength=int(samples/2), \n",
    "                F1=8, D=2, F2=16, dropoutType = 'Dropout')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (18, 4, 888, 1)\n",
      "X_test.shape = (3, 4, 888, 1)\n",
      "X_val.shape = (3, 4, 888, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[:18]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "print('X_train.shape = {}'.format(X_train.shape))\n",
    "\n",
    "X_test = data[18:21]\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "print('X_test.shape = {}'.format(X_test.shape))\n",
    "\n",
    "X_val = data[21:24]\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "print('X_val.shape = {}'.format(X_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train.shape = (18, 3)\n",
      "Y_test.shape = (3, 3)\n",
      "Y_val.shape = (3, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import utils as np_utils\n",
    "Y_train = labels[:18]\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes=3)\n",
    "print('Y_train.shape = {}'.format(Y_train.shape))\n",
    "\n",
    "\n",
    "Y_test = labels[18:21]\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes=3)\n",
    "print('Y_test.shape = {}'.format(Y_test.shape))\n",
    "\n",
    "Y_val = labels[21:24]\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes=3)\n",
    "print('Y_val.shape = {}'.format(Y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 - 0s - loss: 0.7189 - accuracy: 0.7222 - val_loss: 1.1016 - val_accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "2/2 - 0s - loss: 0.7414 - accuracy: 0.7222 - val_loss: 1.1017 - val_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "2/2 - 0s - loss: 0.6490 - accuracy: 0.7222 - val_loss: 1.1023 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "2/2 - 0s - loss: 0.6755 - accuracy: 0.7222 - val_loss: 1.1057 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "2/2 - 0s - loss: 0.6400 - accuracy: 0.8333 - val_loss: 1.1034 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc2152cf10>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 16, epochs=5,\n",
    "         verbose = 2, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.333333 \n"
     ]
    }
   ],
   "source": [
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
